<!DOCTYPE html>
<html lang="en">

  <head>
    <title>osmos::feed</title>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="robots" content="noindex, nofollow" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico" />
    <link rel="alternate" type="application/rss+xml" title="osmos::feed" href="feed.atom" />
    <link href="index.css?v1.14.4" rel="stylesheet" />
    <!-- %before-head-end.html% -->
  </head>

  <body>
    <!-- %after-body-begin.html% -->
      <section class="daily-content js-toggle-accordions-scope">
        <h2 class="daily-heading">
          <button
            class="daily-heading-toggle"
            data-action="toggle-accordions"
            title="Click to toggle the day, Ctrl + click to toggle all."
          >
            <time class="daily-heading-toggle__weekday js-offset-weekday" data-offset-date=2023-03-23 datetime="2023-03-23T05:32:41.000Z">2023-03-23</time>
            <time class="daily-heading-toggle__date js-offset-date" data-offset-date=2023-03-23 datetime="2023-03-23T05:32:41.000Z"></time>
          </button>
        </h2>
        <ul class="sources card">
            <li class="card__section">
              <section class="js-toggle-accordions-scope">
                <h3 class="source-heading">
                  <button
                    class="source-heading__name"
                    data-action="toggle-accordions"
                    title="Click to toggle the source, Ctrl + click to toggle all."
                  >null program</button>
                  <a class="source-heading__link" href="https://nullprogram.com">Open</a>
                </h3>
                <section class="articles-per-source">
                    <article>
                      <details
                        class="article-expander"
                        data-action="toggle-native-accordion"
                        data-accordion-key="https://nullprogram.com/blog/2023/03/23/"
                        open
                      >
                        <summary class="article-expander__title">Practical libc-free threading on Linux</summary>
                        <a class="article-summary-link article-summary-box-outer" href="https://nullprogram.com/blog/2023/03/23/">
                          <div class="article-summary-box-inner media-object">
                            <span class="media-object__text">
                              <span>Suppose you’re not using a C runtime on Linux, and instead you’re
programming against its system call API. It’s long-term and stable after
all. Memory management and buffered I/O are easily
solved, but a lot of software benefits from concurrency. It would be nice
to also have thread spawning capability. This article will demonstrate a
simple, practical, and robust approach to spawning and managing threads
using only raw system calls. It only takes about a dozen lines of C,
including 4 or so inline assembly instructions.
The catch is that there’s no way to avoid using a bit of assembly. Neither
the clone nor clone3 system calls have threading semantics compatible
with C, so you’ll need to paper over it with a bit of inline assembly per
architecture. This article will focus on x86-64, but th…</span>
                                &ensp;<span class="article-reading-time">(&hairsp;7
                                  min&hairsp;)</span>
                            </span>
                          </div>
                        </a>
                      </details>
                    </article>
                </section>
              </section>
            </li>
        </ul>
      </section>
      <section class="daily-content js-toggle-accordions-scope">
        <h2 class="daily-heading">
          <button
            class="daily-heading-toggle"
            data-action="toggle-accordions"
            title="Click to toggle the day, Ctrl + click to toggle all."
          >
            <time class="daily-heading-toggle__weekday js-offset-weekday" data-offset-date=2023-03-19 datetime="2023-03-19T00:00:00.000Z">2023-03-19</time>
            <time class="daily-heading-toggle__date js-offset-date" data-offset-date=2023-03-19 datetime="2023-03-19T00:00:00.000Z"></time>
          </button>
        </h2>
        <ul class="sources card">
            <li class="card__section">
              <section class="js-toggle-accordions-scope">
                <h3 class="source-heading">
                  <button
                    class="source-heading__name"
                    data-action="toggle-accordions"
                    title="Click to toggle the source, Ctrl + click to toggle all."
                  >Max Slater</button>
                  <a class="source-heading__link" href="https://thenumbat.github.io/">Open</a>
                </h3>
                <section class="articles-per-source">
                    <article>
                      <details
                        class="article-expander"
                        data-action="toggle-native-accordion"
                        data-accordion-key="https://thenumbat.github.io/Hashtables/"
                        open
                      >
                        <summary class="article-expander__title">Optimizing Open Addressing</summary>
                        <a class="article-summary-link article-summary-box-outer" href="https://thenumbat.github.io/Hashtables/">
                          <div class="article-summary-box-inner media-object">
                            <span class="media-object__text">
                              <span>Your default hash table should be open-addressed, using Robin Hood linear probing with backward-shift deletion. When prioritizing deterministic performance over memory efficiency, two-way chaining is also a good choice. Code for this article may be found on GitHub. Open Addressing vs. Separate Chaining Benchmark Setup Discussion Separate Chaining Linear Probing Quadratic Probing Double Hashing Robin Hood Linear Probing Two Way Chaining Unrolling, Prefetching, and SIMD Benchmark Data Open Addressing vs. Separate Chaining Most people first encounter hash tables implemented using separate chaining, a model simple to understand and analyze mathematically. In separate chaining, a hash function is used to map each key to one of \(K\) buckets. Each bucket holds a linked list, so to retrieve a key, one simply traverses its corresponding bucket. Given a hash function drawn from a universal family, inserting \(N\) keys into a table with \(K\) buckets results in an expected bucket size of \(\frac{N}{K}\), a quantity also known as the table’s load factor. Assuming (reasonably) that \(\frac{N}{K}\) is bounded by a constant, all operations on such a table can be performed in expected constant time. Unfortunately, this basic analysis doesn’t consider the myriad factors that go into implementing an efficient hash table on a real computer. In practice, hash tables based on open addressing can provide superior performance, and their limitations can be worked around in nearly all cases. Open Addressing In an open-addressed table, each bucket only contains a single key. Collisions are handled by placing additional keys elsewhere in the table. For example, in linear probing, a key is placed in the first open bucket starting from the index it hashes to. Unlike in separate chaining, open-addressed tables may be represented in memory as a single flat array. A priori, we should expect operations on flat arrays to offer higher performance than those on linked structures due to more coherent memory accesses.1 However, if the user wants to insert more than \(K\) keys into an open-addressed table with \(K\) buckets, the entire table must be resized. Typically, a larger array is allocated and all current keys are moved to the new table. The size is grown by a constant factor (e.g. 2x), so insertions occur in amortized constant time. Why Not Separate Chaining? In practice, the standard libraries of many languages provide separate chaining based hash tables, such as C++’s std::unordered_map. At least in C++, this choice is now considered a mistake—it violates the principle of not paying for features you didn’t ask for. Separate chaining still has some purported benefits, but they seem unconvincing: Separately chained tables don’t require any linear-time operations. First, this isn’t true. To maintain a constant load factor, separate chaining hash tables also have to resize once a sufficient number of keys are inserted, though the limit can be greater than \(K\). Most separately chained tables only provide amortized constant time insertion. Second, open-addressed tables can be incrementally resized. When the table grows, there’s no reason we have to move all existing keys right away. Instead, every subsequent operation can move a fixed number of old keys into the new table. Eventually, all keys will have been moved and the old table may be deallocated. This technique incurs overhead on every operation during a resize, but none will require linear time. When each (key + value) entry is allocated separately, entries can have stable addresses. External code can safely store pointers to them, and large entries never have to be copied. This property is easy to support by adding a single level of indirection. That is, allocate each entry separately, but use an open-addressed table to map keys to pointers-to-entries. In this case, resizing the table only requires copying pointers, rather than entire entries. Lower memory overhead. It’s possible for a separately chained table to store the same data in less space than an open-addressed equivalent, since flat tables necessarily waste space storing empty buckets. However, matching high-quality open addressing schemes (especially when storing entries indirectly) requires a load factor greater than one, degrading query performance. Further, individually allocating nodes wastes memory due to heap allocator overhead and fragmentation. In fact, the only situation where open addressing truly doesn’t work is when the table cannot allocate and entries cannot be moved. These requirements can arise when using intrusive linked lists, a common pattern in kernel data structures and embedded systems with no heap. Benchmark Setup For simplicity, let us only consider tables mapping 64-bit integer keys to 64-bit integer values. Results will not take into account the effects of larger values or non-trivial key comparison, but should still be representative, as keys can often be represented in 64 bits and values are often pointers. We will evaluate tables using the following metrics: Time to insert \(N\) keys into an empty table. Time to erase \(N\) existing keys. Time to look up \(N\) existing keys. Time to look up \(N\) missing keys2. Average probe length for existing &amp; missing keys. Maximum probe length for existing &amp; missing keys. Memory amplification in a full table (total size / size of data). Each open-addressed table was benchmarked at 50%, 75%, and 90% load factors. Every test targeted a table capacity of 8M entries, setting \(N &#x3D; \lfloor 8388608 * \text{Load Factor} \rfloor - 1\). Fixing the table capacity allows us to benchmark behavior at exactly the specified load factor, avoiding misleading results from tables that have recently grown. The large number of keys causes each test to unpredictably traverse a data set much larger than L3 cache (128MB+), so the CPU3 cannot effectively cache or prefetch memory accesses. Table Sizes All benchmarked tables use exclusively power-of-two sizes. This invariant allows us to translate hashes to array indices with a fast bitwise AND operation, since: h % 2**n &#x3D;&#x3D; h &amp; (2**n - 1) Power-of-two sizes also admit a neat virtual memory trick4, but that optimization is not included here. However, power-of-two sizes can cause pathological behavior when paired with some hash functions, since indexing simply throws away the hash’s high bits. An alternative strategy is to use prime sized tables, which are significantly more robust to the choice of hash function. For the purposes of this post, we have a fixed hash function, so prime sizes were not used5. Hash Function All benchmarked tables use the squirrel3 hash function, a fast integer hash that (as far as I can tell) only exists in this GDC talk on noise-based RNG. Here, it is adapted to 64-bit integers by choosing three large 64-bit primes: uint64_t squirrel3(uint64_t at) { constexpr uint64_t BIT_NOISE1 &#x3D; 0x9E3779B185EBCA87ULL; constexpr uint64_t BIT_NOISE2 &#x3D; 0xC2B2AE3D27D4EB4FULL; constexpr uint64_t BIT_NOISE3 &#x3D; 0x27D4EB2F165667C5ULL; at *&#x3D; BIT_NOISE1; at ^&#x3D; (at &gt;&gt; 8); at +&#x3D; BIT_NOISE2; at ^&#x3D; (at &lt;&lt; 8); at *&#x3D; BIT_NOISE3; at ^&#x3D; (at &gt;&gt; 8); return at; } I make no claims regarding the quality or robustness of this hash function, but observe that it’s cheap, it produces the expected number of collisions in power-of-two tables, and it passes smhasher when applied bytewise. Other Benchmarks Several less interesting benchmarks were also run, all of which exhibited identical performance across equal-memory open-addressed tables and much worse performance for separate chaining. Time to look up each element by following a Sattolo cycle6 (2-4x). Time to clear the table (100-200x). Time to iterate all values (3-10x). Discussion Separate Chaining To get our bearings, let’s first implement a simple separate chaining table and benchmark it at various load factors. We won’t be able to directly compare these results against the open addressed tables, but we can get a sense of what we’re up against. Separate Chaining50% Load100% Load200% Load500% Load ExistingMissingExistingMissingExistingMissingExistingMissing Insert (ns/key)120130130151 Erase (ns/key)112135179305 Lookup (ns/key)282836415169103186 Average Probe1.250.501.501.002.002.003.505.00 Max Probe77101012122121 Memory Amplification2.502.001.751.60 These are respectable results, especially lookups at 100%: the CPU is able to hide most of the memory latency despite the double indirection. Insertions and erasures, on the other hand, are pretty slow—they involve allocation. Technically, the reported memory amplification in an underestimate, as it does not include heap allocator overhead. However, we should not directly compare memory amplification against the open-addressed tables, as storing larger values would improve separate chaining’s ratio. std::unordered_map Running these benchmarks on std::unordered_map&lt;uint64_t,uint64_t&gt; produces roughly the same results as the 100% column, though with slower insertions and faster clears. Using squirrel3 with std::unordered_map additionally makes insertions slightly slower and lookups slightly faster. However, these results should not be considered canonical, since they depend on your standard library distribution (here, Microsoft’s). Linear Probing Next, let’s implement the simplest type of open addressing—linear probing. In the following diagrams, assume that each letter hashes to its alphabetical index. The skull denotes a tombstone. Insert: starting from the key’s index, place the key in the first empty or tombstone bucket. Lookup: starting from the key’s index, probe buckets in order until finding the key, reaching an empty non-tombstone bucket, or exhausting the table. Erase: lookup the key and replace it with a tombstone. The results are surprisingly good, considering the simplicity: Linear Probing(Naive)50% Load75% Load90% Load ExistingMissingExistingMissingExistingMissing Insert (ns/key)476073 Erase (ns/key)213647 Lookup (ns/key)21863512247291 Average Probe0.506.041.4929.494.46222.04 Max Probe438518143816042816 Memory Amplification2.001.331.11 For existing keys, we’re already beating the separately chained tables, and with lower memory overhead! Of course, there are also some obvious problems—looking for missing keys takes much longer, and the worst-case probe lengths are quite scary, even at 50% load factor. But, we can do much better. Erase: Rehashing One simple optimization is automatically recreating the table once it accumulates too many tombstones. Erase now occurs in amortized constant time, but we already tolerate that for insertion, so it shouldn’t be a big deal. Let’s implement a table that re-creates itself after erase has been called \(\frac{N}{2}\) times. Compared to naive linear probing: Linear Probing+ Rehashing50% Load75% Load90% Load ExistingMissingExistingMissingExistingMissing Insert (ns/key)465770 Erase (ns/key)23 (+8.3%)27 (-24.6%)28 (-39.7%) Lookup (ns/key)20843489 (-26.8%)46142 (-51.2%) Average Probe0.506.041.499.34 (-68.3%)4.4657.84 (-74.0%) Max Probe4385181245 (-44.1%)16041405 (-50.1%) Memory Amplification2.001.331.11 Clearing the tombstones dramatically improves missing key queries, but they are still pretty slow. It also makes erase slower at 50% load—there aren’t enough tombstones to make up for having to recreate the table. Rehashing also does nothing to mitigate long probe sequences for existing keys. Erase: Backward Shift Actually, who says we need tombstones? There’s a little-taught, but very simple algorithm for erasing keys that doesn’t degrade performance or have to recreate the table. When we erase a key, we don’t know whether the lookup algorithm is relying on our bucket being filled to find keys later in the table. Tombstones are one way to avoid this problem. Instead, however, we can guarantee that traversal is never disrupted by simply removing and reinserting all following keys, up to the next empty bucket. Note that despite the name, we are not simply shifting the following keys backward. Any keys that are already at their optimal index will not be moved. For example: After implementing backward-shift deletion, we can compare it to linear probing with rehashing: Linear Probing+ Backshift50% Load75% Load90% Load ExistingMissingExistingMissingExistingMissing Insert (ns/key)486073 Erase (ns/key)47 (+107%)83 (+208%)149 (+425%) Lookup (ns/key)2038 (-55.1%)368946134 Average Probe0.501.50 (-75.2%)1.497.46 (-20.1%)4.4649.7 (-14.1%) Max Probe4350 (-41.2%)18124316041403 Memory Amplification2.001.331.11 Naturally, erasures become significantly slower, but queries on missing keys now have reasonable behavior, especially at 50% load. In fact, at 50% load, this table already beats the performance of equal-memory separate chaining in all four metrics. But, we can still do better: a 50% load factor is unimpressive, and max probe lengths are still very high compared to separate chaining. You might have noticed that neither of these upgrades improved average probe length for existing keys. That’s because it’s not possible for linear probing to have any other average! Think about why that is—it’ll be interesting later. Quadratic Probing Quadratic probing is a common upgrade to linear probing intended to decrease average and maximum probe lengths. In the linear case, a probe of length \(n\) simply queries the bucket at index \(h(k) + n\). Quadratic probing instead queries the bucket at index \(h(k) + n^2\). Each table operation must be updated to use this new probe sequence, but the logic is otherwise almost identical. There are also a couple subtle caveats to quadratic probing: A quadratic probe sequence will not necessarily touch every bucket in the table, so insertions can fail despite there being an empty bucket. In this case, the table must grow and the insertion is retried. There actually is a true deletion algorithm for quadratic probing, but it’s a bit more involved than the linear case and hence not reproduced here. We will again use tombstones and rehashing. After implementing our quadratic table, we can compare it to linear probing with rehashing: Quadratic Probing+ Rehashing50% Load75% Load90% Load ExistingMissingExistingMissingExistingMissing Insert (ns/key)506072 Erase (ns/key)31 (+36.6%)36 (+32.0%)38 (+32.0%) Lookup (ns/key)21863297 (+8.0%)43179 (+25.8%) Average Probe0.43 (-13.9%)4.75 (-21.4%)0.97 (-35.2%)5.15 (-44.9%)1.79 (-59.9%)17.53 (-69.7%) Max Probe19 (-55.8%)56 (-34.1%)47 (-74.0%)87 (-64.5%)112 (-93.0%)289 (-79.4%) Memory Amplification2.001.331.11 As expected, quadratic probing dramatically reduces both the average and worst case probe lengths, especially at high load factors. These gains are not quite as impressive when compared to the linear table with backshift deletion, but are still very significant. Reducing the average probe length improves the average performance of all queries. Here, however, query performance has regressed slightly, as quadratic probing itself requires more computation and accesses memory less predictably. That’s not necessarily a deal breaker—performance is lower variance—but it’s not ideal. Double Hashing Double hashing purports to provide even better behavior than quadratic probing. Instead of using the same probe sequence for every key, double hashing determines the probe stride by hashing the key a second time. That is, a probe of length \(n\) queries the bucket at index \(h_1(k) + n*h_2(k)\) If \(h_2\) is always co-prime to the table size, insertions always succeed, since the probe sequence will touch every bucket. That might sound difficult to assure, but since our table sizes are always a power of two, we can simply make \(h_2\) always return an odd number. Unfortunately, there is no true deletion algorithm for double hashing. We will again use tombstones and rehashing. Let’s implement a table with a slightly modified squirrel3 as the second hash. The results, compared to quadratic probing with rehashing: Double Hashing+ Rehashing50% Load75% Load90% Load ExistingMissingExistingMissingExistingMissing Insert (ns/key)72 (+42.8%)77 (+28.9%)87 (+20.8%) Erase (ns/key)3040 (+11.4%)49 (+31.8%) Lookup (ns/key)28 (+33.2%)9038 (+18.6%)9247(+8.6%)180 Average Probe0.39 (-10.1%)4.38 (-7.7%)0.85 (-12.3%)4.72 (-8.3%)1.56 (-13.0%)16.25 (-7.3%) Max Probe17(-10.5%)62 (+10.7%)42(-10.6%)66 (-24.1%)116344 (+19.0%) Memory Amplification2.001.331.11 Double hashing succeeds in further reducing average probe lengths, but max lengths are more of a mixed bag. Unfortunately, the additional overhead of hashing every key twice makes most queries significantly slower, so double hashing does not seem worth it. Robin Hood Linear Probing So far, quadratic probing and double hashing have provided low probe lengths, but their raw query performance lags linear probing with backward shift deletion—at least at low load factors. Enter Robin Hood linear probing. This strategy drastically reins in maximum probe lengths while maintaining high query performance. The key difference is that when inserting a key, we are allowed to steal the position of an existing key if it is closer to its ideal index (“richer”) than the new key is. When this occurs, we simply swap the old and new keys and carry on inserting the old key in the same manner. This results in a more equitable distribution of probe lengths. This insertion method turns out to be so effective that we can entirely ignore rehashing as long as we keep track of the maximum probe length. Lookups will simply probe until either finding the key or exhausting the maximum probe length. Implementing this table, compared to linear probing with backshift deletion: Robin Hood(Naive)50% Load75% Load90% Load ExistingMissingExistingMissingExistingMissing Insert (ns/key)55 (+14.6%)85 (+41.8%)128 (+74.5%) Erase (ns/key)20 (-58.3%)31 (-63.0%)78 (-47.7%) Lookup (ns/key)20 58 (+54.5%)32 (-11.3%)109 (+23.1%)81 (+74.4%)148 (+9.9%) Average Probe0.5013 (+769%)1.4928(+275%)4.4667 (+34.9%) Max Probe12 (-72.1%)13 (-74.0%)24 (-86.7%)28(-88.5%)58 (-96.4%)67 (-95.2%) Memory Amplification2.001.331.11 Maximum probe lengths improve dramatically, undercutting even quadratic probing and double hashing. This solves the main problem with open addressing. However, performance gains are less clear-cut: Insertion performance suffers across the board. Existing lookups are slightly faster at low load, but much slower at 90%. Missing lookups are maximally pessimistic. Fortunately, we’ve got one more trick up our sleeves. Erase: Backward Shift When erasing a key, we can run a very similar backshift deletion algorithm. There are just two differences: We can stop upon finding a key that is already in its optimal location. No further keys could have been pushed past it—they would have stolen this spot during insertion. We don’t have to re-hash—since we stop at the first optimal key, all keys before it can simply be shifted backward. With backshift deletion, we no longer need to keep track of the maximum probe length. Implementing this table, compared to naive robin hood probing: Robin Hood+ Backshift50% Load75% Load90% Load ExistingMissingExistingMissingExistingMissing Insert (ns/key)5582125 Erase (ns/key)32 (+64.0%)48 (+54.2%)60 (-22.8%) Lookup (ns/key)2034 (-42.0%)3276 (-30.9%)80114 (-22.8%) Average Probe0.500.75 (-94.2%)1.491.87 (-93.3%)4.464.95 (-92.6%) Max Probe1212 (-7.7%)2425 (-10.7%)5867 Memory Amplification2.001.331.11 We again regress the performance of erase, but we finally achieve reasonable missing-key behavior. Given the excellent maximum probe lengths and overall query performance, Robin Hood probing with backshift deletion should be your default choice of hash table. You might notice that at 90% load, lookups are significantly slower than basic linear probing. This gap actually isn’t concerning, and explaining why is an interesting exercise7. Two-Way Chaining So, you’re still not satisfied with a maximum observed probe length of 12—you want to provably never iterate more than a constant number of buckets. Another class of hash tables based on Cuckoo hashing can provide this guarantee. Here, we will examine a particularly practical formulation known as Two-Way Chaining. Our table will still consist of a flat array of buckets, but each bucket will be able to hold a small handful of keys. When inserting an element, we will hash the key with two different functions, placing it into whichever corresponding bucket has more space. If both buckets happen to be filled, the entire table will grow and the insertion is retried. You might think this is crazy—in separate chaining, the expected maximum probe length scales with \(O(\frac{\lg N}{\lg \lg N})\), so wouldn’t this waste copious amounts of memory growing the table? It turns out adding a second hash function reduces the expected maximum to \(O(\lg\lg N)\), for reasons explored elsewhere. That makes it practical to cap the maximum bucket size at a relatively small number. Let’s implement a flat two-way chained table. We can measure how it performs with various bucket capacities: Two-WayChainingCapacity 2Capacity 4Capacity 8 ExistingMissingExistingMissingExistingMissing Insert (ns/key)165126123 Erase (ns/key)334254 Lookup (ns/key)323141354646 Average Probe0.070.470.752.733.608.96 Max Probe34681214 Memory Amplification16.004.002.00 Since every key can be found in one of two possible buckets, query times become essentially deterministic. Lookup times are especially impressive, only slightly lagging the Robin Hood table and having no penalty on missing keys. The average probe lengths indicate that most buckets are approximately half full, and the max probe lengths are strictly bounded by two times the bucket size. Memory efficiency does suffer at small bucket capacities (and hence insertion is slower on average due to growing the table), but is still reasonable for memory unconstrained use cases. Overall, two-way chaining is another good choice of hash table, at least when memory efficiency is not the highest priority. In the next section, we’ll also see how lookups can even further accelerated with prefetching and SIMD operations. Remember that deterministic queries might not matter if you haven’t already implemented incremental table growth. Cuckoo Hashing Cuckoo hashing involves storing keys in two separate tables, each of which holds one key per bucket. This scheme will guarantee that every key is stored in its ideal bucket in one of the two tables. Each key is hashed with two different functions and inserted using the following procedure: If at least one table’s bucket is empty, the key is inserted there. Otherwise, the key is inserted anyway, evicting one of the existing keys. The evicted key is transferred to the opposite table at its other possible position. If doing so evicts another existing key, go to 3. If the loop ends up evicting the key we’re trying to insert, we know it has found a cycle and hence must grow the table. Because Cuckoo hashing does not strictly bound the insertion sequence length, I didn’t benchmark it here, but it’s still an interesting option. Unrolling, Prefetching, and SIMD So far, the lookup benchmark has been implemented roughly like this: for(uint64_t i &#x3D; 0; i &lt; N; i++) { assert(table.find(keys[i]) &#x3D;&#x3D; values[i]); } Naively, this loop runs one lookup at a time. However, you might have noticed that we’ve been able to find keys faster than the CPU can load data from main memory—so something more complex must be going on. In reality, modern out-of-order CPUs can execute several iterations in parallel. Expressing the loop as a dataflow graph lets us build a simplified mental model of how it actually runs: the CPU is able to execute a computation whenever its dependencies are available. Here, green nodes are resolved, in-flight nodes are yellow, and white nodes have not begun: In this example, we can see that each iteration’s root node (i++) only requires the value of i from the previous step—not the result of find. Therefore, the CPU can run several find nodes in parallel, hiding the fact that each one takes ~100ns to fetch data from main memory. Unrolling There’s a common optimization strategy known as loop unrolling. Unrolling involves explicitly writing out the code for several semantic iterations inside each actual iteration of a loop. When each inner pseudo-iteration is independent, unrolling explicitly severs them from the loop iterator dependency chain. Most modern x86 CPUs can keep track of ~10 simultaneous pending loads, so let’s try manually unrolling our benchmark 10 times. The new code looks like the following, where index_of hashes the key but doesn’t do the actual lookup. uint64_t indices[10]; for(uint64_t i &#x3D; 0; i &lt; N; i +&#x3D; 10) { for(uint64_t j &#x3D; 0; j &lt; 10; j++) { indices[j] &#x3D; table.index_of(keys[i + j]); } for(uint64_t j &#x3D; 0; j &lt; 10; j++) { assert(table.find_index(indices[j]) &#x3D;&#x3D; values[i + j]); } } This isn’t true unrolling, since the inner loops introduce dependencies on j. However, it doesn’t matter in practice, as computing j is never a bottleneck. In fact, our compiler can automatically unroll the inner loop for us (clang in particular does this). Lookups (ns/find)NaiveUnrolled Separate Chaining (100%)3635 Linear (75%)3535 Quadratic (75%)3232 Double (75%)3837 Robin Hood (75%)3232 Two-Way Chaining (x4)4145 (+11.4%) Explicit unrolling has basically no effect, since the CPU was already able to saturate its pending load buffer. However, manual unrolling now lets us introduce software prefetching. Software Prefetching When traversing memory in a predictable pattern, the CPU is able to preemptively load data for future accesses. This capability, known as hardware prefetching, is the reason our flat tables are so fast to iterate. However, hash table lookups are inherently unpredictable: the address to load is determined by a hash function. That means hardware prefetching cannot help us. Instead, we can explicitly ask the CPU to prefetch address(es) we will load from in the near future. Doing so can’t speed up an individual lookup—we’d immediately wait for the result—but software prefetching can accelerate batches of queries. Given several keys, we can compute where each query will access the table, tell the CPU to prefetch those locations, and then start actually accessing the table. To do so, let’s make table.index_of prefetch the location(s) examined by table.find_indexed: In open-addressed tables, we prefetch the slot the key hashes to. In two-way chaining, we prefetch both buckets the key hashes to. Importantly, prefetching requests the entire cache line for the given address, i.e. the aligned 64-byte chunk of memory containing that address. That means surrounding data (e.g. the following keys) may also be loaded into cache. Lookups (ns/find)NaiveUnrolledPrefetched Separate Chaining (100%)363527 (-25.6%) Linear (75%)353523 (-35.5%) Quadratic (75%)323224 (-24.8%) Double (75%)383731 (-17.4%) Robin Hood (75%)323223 (-28.3%) Two-Way Chaining (x4)4145 (+11.4%)29 (-29.1%) Prefetching makes a big difference! Separate chaining benefits from prefetching the unpredictable initial load8. Linear tables with a low average probe length greatly benefit from prefetching. Quadratic and double-hashed probe sequences quickly leave the cache line (and require more computation), making prefetching a bit less effective. In two-way chaining, keys can always be found in a prefetched cache line—but each lookup requires fetching two locations, consuming extra load buffer slots. SIMD Modern CPUs provide SIMD (single instruction, multiple data) instructions that process multiple values at once. These operations are useful for probing: for example, we can load a batch of \(N\) keys and compare them with our query in parallel. In the best case, a SIMD probe sequence could run \(N\) times faster than checking each key individually. Let’s implement SIMD-accelerated lookups for linear probing and two-way chaining, with \(N&#x3D;4\): Lookups (ns/find)NaiveUnrolledPrefetched Linear (75%)353523 SIMD Linear (75%)40 (+13.7%)43 (+20.7%)27 (+20.1%) Two-Way Chaining (x4)414529 SIMD Two-Way Chaining (x4)34 (-17.2%)35 (-21.9%)19 (-34.8%) Unfortunately, SIMD lookups are slower for the linearly probed table—an average probe length of \(1.5\) meant the overhead of setting up an (unaligned) SIMD comparison wasn’t worth it. Two-way chaining, on the other hand, gets quite a bit faster. Buckets are on average half full, and we can find a key using at most two (aligned) SIMD comparisons. Benchmark Data The linear and Robin Hood tables use backshift deletion. The two-way SIMD table has capacity-4 buckets and uses AVX2 256-bit SIMD operations. Insert, erase, find, find unrolled, find prefetched, and find missing report nanoseconds per operation. Average probe, max probe, average missing probe, and max missing probe report number of iterations. Clear and iterate report milliseconds for the entire operation. Memory reports the ratio of allocated memory to size of stored data. Table Insert Erase Find Unrolled Prefetched Avg Probe Max Probe Find DNE DNE Avg DNE Max Clear (ms) Iterate (ms) Memory Chaining 50 120 112 28 27 21 1.2 7 28 0.5 7 117.1 16.4 2.5 Chaining 100 130 135 36 35 27 1.5 10 41 1.0 9 120.1 14.0 2.0 Chaining 200 130 179 51 51 38 2.0 12 69 2.0 13 133.7 14.6 1.8 Chaining 500 151 305 103 109 77 3.5 21 186 5.0 21 157.2 22.9 1.6 Linear 50 48 47 20 20 16 0.5 43 38 1.5 50 1.1 4.8 2.0 Linear 75 60 83 36 36 23 1.5 181 89 7.5 243 0.8 2.4 1.3 Linear 90 73 149 46 46 30 4.5 1604 134 49.7 1403 0.6 1.0 1.1 Quadratic 50 50 31 21 20 16 0.4 19 86 4.7 56 1.1 5.2 2.0 Quadratic 75 60 36 32 32 24 1.0 47 97 5.1 87 0.7 2.1 1.3 Quadratic 90 72 38 43 43 32 1.8 112 179 17.5 289 0.6 1.0 1.1 Double 50 72 31 28 28 24 0.4 17 90 4.4 62 1.1 5.6 2.0 Double 75 77 41 38 37 31 0.8 42 92 4.7 66 0.7 2.3 1.3 Double 90 87 46 47 48 42 1.6 116 180 16.2 344 0.6 0.9 1.1 Robin Hood 50 55 32 20 19 15 0.5 12 34 0.7 12 1.1 4.8 2.0 Robin Hood 75 82 48 32 32 22 1.5 24 76 1.9 25 0.7 2.1 1.3 Robin Hood 90 125 60 80 78 41 4.5 58 114 5.0 67 0.6 1.2 1.1 Two-way x2 165 33 32 30 19 0.1 3 31 0.5 4 9.0 11.8 16.0 Two-way x4 126 42 41 45 29 0.7 6 35 2.7 8 2.3 3.6 4.0 Two-way x8 123 54 46 50 40 3.6 12 46 9.0 14 1.1 1.5 2.0 Two-way SIMD 152 62 34 35 19 0.3 1 25 1.0 1 2.2 3.6 4.0 Footnotes Actually, this prior shouldn’t necessarily apply to hash tables, which by definition have unpredictable access patterns. However, we will see that flat storage still provides performance benefits, especially for linear probing, whole-table iteration, and serial accesses. ↩ To account for various deletion strategies, the find-missing benchmark was run after the insertion &amp; erasure tests, and after inserting a second set of elements. Looking up the latter set of elements was also benchmarked, but results are omitted here, as the tombstone-based tables were simply marginally slower across the board. ↩ AMD 5950X @ 4.4 GHz + DDR4 3600/CL16, Windows 11 Pro 22H2, MSVC 19.34.31937. No particular effort was made to isolate the benchmarks, but they were repeatable on an otherwise idle system. Each was run 10 times and average results are reported. ↩ Virtual memory allows separate regions of our address space to refer to the same underlying pages. If we map our table twice, one after the other, probe traversal never has to check whether it needs to wrap around to the start of the table. Instead, it can simply proceed into the second mapping—since writes to one range are automatically reflected in the other, this just works.  ↩ You might expect indexing a prime-sized table to require an expensive integer mod/div, but it actually doesn’t. Given a fixed set of possible primes, a prime-specific indexing operation can be selected at runtime based on the current size. Since each such operation is a modulo by a constant, the compiler can translate it to a fast integer multiplication. ↩ A Sattolo traversal causes each lookup to directly depend on the result of the previous one, serializing execution and preventing the CPU from hiding memory latency. Hence, finding each element took ~100ns in every type of open table: roughly equal to the latency of fetching data from RAM. Similarly, each lookup in the low-load separately chained table took ~200ns, i.e. \((1 + \text{Chain Length}) * \text{Memory Latency}\). ↩ First, any linearly probed table requires the same total number of probes to look up each element once—that’s why the average probe length never improved. Every hash collision requires moving some key one slot further from its index, i.e. adds one to the total probe count. Second, the cost of a lookup, per probe, can decrease with the number of probes required. When traversing a long chain, the CPU can avoid cache misses by prefetching future accesses. Therefore, allocating a larger portion of the same total probe count to long chains can increase performance. Hence, linear probing tops this particular benchmark—but it’s not the most useful metric in practice. Robin Hood probing greatly decreases maximum probe length, making query performance far more consistent even if technically slower on average. Anyway, you really don’t need a 90% load factor—just use 75% for consistently high performance. ↩ Is the CPU smart enough to automatically prefetch addresses contained within the explicitly prefetched cache line? I’m pretty sure it’s not, but that would be cool. ↩</span>
                                &ensp;<span class="article-reading-time">(&hairsp;17
                                  min&hairsp;)</span>
                            </span>
                          </div>
                        </a>
                      </details>
                    </article>
                </section>
              </section>
            </li>
        </ul>
      </section>
      <section class="daily-content js-toggle-accordions-scope">
        <h2 class="daily-heading">
          <button
            class="daily-heading-toggle"
            data-action="toggle-accordions"
            title="Click to toggle the day, Ctrl + click to toggle all."
          >
            <time class="daily-heading-toggle__weekday js-offset-weekday" data-offset-date=2023-03-14 datetime="2023-03-14T15:48:28.000Z">2023-03-14</time>
            <time class="daily-heading-toggle__date js-offset-date" data-offset-date=2023-03-14 datetime="2023-03-14T15:48:28.000Z"></time>
          </button>
        </h2>
        <ul class="sources card">
            <li class="card__section">
              <section class="js-toggle-accordions-scope">
                <h3 class="source-heading">
                  <button
                    class="source-heading__name"
                    data-action="toggle-accordions"
                    title="Click to toggle the source, Ctrl + click to toggle all."
                  >Netflix TechBlog - Medium</button>
                  <a class="source-heading__link" href="https://netflixtechblog.com?source&#x3D;rss----2615bd06b42e---4">Open</a>
                </h3>
                <section class="articles-per-source">
                    <article>
                      <details
                        class="article-expander"
                        data-action="toggle-native-accordion"
                        data-accordion-key="https://medium.com/p/9bef9962dcb7"
                        open
                      >
                        <summary class="article-expander__title">Building a Media Understanding Platform for ML Innovations</summary>
                        <a class="article-summary-link article-summary-box-outer" href="https://netflixtechblog.com/building-a-media-understanding-platform-for-ml-innovations-9bef9962dcb7?source&#x3D;rss----2615bd06b42e---4">
                          <div class="article-summary-box-inner media-object">
                              <img src="https://miro.medium.com/v2/resize:fit:854/0*6aJVHbaKKjAKvc3X" loading="lazy" class="media-object__media article-image" />
                            <span class="media-object__text">
                              <span>The media understanding platform serves as an abstraction layer between Machine Learning (ML) algos and various applications.</span>
                                &ensp;<span class="article-reading-time">(&hairsp;21
                                  min&hairsp;)</span>
                            </span>
                          </div>
                        </a>
                      </details>
                    </article>
                </section>
              </section>
            </li>
            <li class="card__section">
              <section class="js-toggle-accordions-scope">
                <h3 class="source-heading">
                  <button
                    class="source-heading__name"
                    data-action="toggle-accordions"
                    title="Click to toggle the source, Ctrl + click to toggle all."
                  >Max Slater</button>
                  <a class="source-heading__link" href="https://thenumbat.github.io/">Open</a>
                </h3>
                <section class="articles-per-source">
                    <article>
                      <details
                        class="article-expander"
                        data-action="toggle-native-accordion"
                        data-accordion-key="https://thenumbat.github.io/Bronx-Zoo/"
                        open
                      >
                        <summary class="article-expander__title">NYC: Bronx Zoo</summary>
                        <a class="article-summary-link article-summary-box-outer" href="https://thenumbat.github.io/Bronx-Zoo/">
                          <div class="article-summary-box-inner media-object">
                            <span class="media-object__text">
                              <span>Bronx Zoo, New York, NY, 2023</span>
                            </span>
                          </div>
                        </a>
                      </details>
                    </article>
                </section>
              </section>
            </li>
        </ul>
      </section>
      <section class="daily-content js-toggle-accordions-scope">
        <h2 class="daily-heading">
          <button
            class="daily-heading-toggle"
            data-action="toggle-accordions"
            title="Click to toggle the day, Ctrl + click to toggle all."
          >
            <time class="daily-heading-toggle__weekday js-offset-weekday" data-offset-date=2023-03-10 datetime="2023-03-10T22:59:11.000Z">2023-03-10</time>
            <time class="daily-heading-toggle__date js-offset-date" data-offset-date=2023-03-10 datetime="2023-03-10T22:59:11.000Z"></time>
          </button>
        </h2>
        <ul class="sources card">
            <li class="card__section">
              <section class="js-toggle-accordions-scope">
                <h3 class="source-heading">
                  <button
                    class="source-heading__name"
                    data-action="toggle-accordions"
                    title="Click to toggle the source, Ctrl + click to toggle all."
                  >Netflix TechBlog - Medium</button>
                  <a class="source-heading__link" href="https://netflixtechblog.com?source&#x3D;rss----2615bd06b42e---4">Open</a>
                </h3>
                <section class="articles-per-source">
                    <article>
                      <details
                        class="article-expander"
                        data-action="toggle-native-accordion"
                        data-accordion-key="https://medium.com/p/99332231e541"
                        open
                      >
                        <summary class="article-expander__title">Elasticsearch Indexing Strategy in Asset Management Platform (AMP)</summary>
                        <a class="article-summary-link article-summary-box-outer" href="https://netflixtechblog.com/elasticsearch-indexing-strategy-in-asset-management-platform-amp-99332231e541?source&#x3D;rss----2615bd06b42e---4">
                          <div class="article-summary-box-inner media-object">
                              <img src="https://miro.medium.com/v2/resize:fit:1200/1*DQfg8USKquc3t9tdayuvQA.png" loading="lazy" class="media-object__media article-image" />
                            <span class="media-object__text">
                              <span>By Burak Bacioglu, Meenakshi Jindal</span>
                                &ensp;<span class="article-reading-time">(&hairsp;18
                                  min&hairsp;)</span>
                            </span>
                          </div>
                        </a>
                      </details>
                    </article>
                    <article>
                      <details
                        class="article-expander"
                        data-action="toggle-native-accordion"
                        data-accordion-key="https://medium.com/p/46fe225c35c9"
                        open
                      >
                        <summary class="article-expander__title">Data Reprocessing Pipeline in Asset Management Platform @Netflix</summary>
                        <a class="article-summary-link article-summary-box-outer" href="https://netflixtechblog.com/data-reprocessing-pipeline-in-asset-management-platform-netflix-46fe225c35c9?source&#x3D;rss----2615bd06b42e---4">
                          <div class="article-summary-box-inner media-object">
                              <img src="https://miro.medium.com/v2/resize:fit:1200/1*1-HX6b39H2L9PwsyRaEBxA.png" loading="lazy" class="media-object__media article-image" />
                            <span class="media-object__text">
                              <span>By Meenakshi Jindal</span>
                                &ensp;<span class="article-reading-time">(&hairsp;14
                                  min&hairsp;)</span>
                            </span>
                          </div>
                        </a>
                      </details>
                    </article>
                </section>
              </section>
            </li>
            <li class="card__section">
              <section class="js-toggle-accordions-scope">
                <h3 class="source-heading">
                  <button
                    class="source-heading__name"
                    data-action="toggle-accordions"
                    title="Click to toggle the source, Ctrl + click to toggle all."
                  >Nim Programming Language</button>
                  <a class="source-heading__link" href="https://nim-lang.org//">Open</a>
                </h3>
                <section class="articles-per-source">
                    <article>
                      <details
                        class="article-expander"
                        data-action="toggle-native-accordion"
                        data-accordion-key="https://nim-lang.org//blog/2023/03/10/version-1612-released.html"
                        open
                      >
                        <summary class="article-expander__title">Version 1.6.12 released</summary>
                        <a class="article-summary-link article-summary-box-outer" href="https://nim-lang.org//blog/2023/03/10/version-1612-released.html">
                          <div class="article-summary-box-inner media-object">
                              <img src="https://nim-lang.org/assets/img/twitter_banner.png" loading="lazy" class="media-object__media article-image" />
                            <span class="media-object__text">
                              <span>The Nim team is happy to announce version 1.6.12, our sixth patch release for
Nim 1.6.
Version 1.6.12 is a result of almost four months of hard work, and it contains
51 commits,
bringing some general improvements over 1.6.10.
We would recommend to all of our users to upgrade and use version 1.6.12.
Installing Nim 1.6
New users
Check out if the package manager of your OS already ships version 1.6.12 or
install it as described here.
Existing users
If you have installed a previous version of Nim using choosenim,
getting Nim 1.6.12 is as easy as:
$ choosenim update stable

Alternatively, you can download Nim 1.6.12 from
our nightlies builds.
Donating to Nim
We would like to encourage you to donate to Nim.
The donated money will be used to further improve Nim by creating bounties
for the most important bugfixes and features.
You can donate via:
Open Collective
Patreon
BountySource
PayPal
Bitcoin: 1BXfuKM2uvoD6mbx4g5xM3eQhLzkCK77tJ
If you are a company, we also offer commercial support.
Bugfixes
These reported issues were fixed:
Fixed “sizeof object containing a set is wrong”
(#20914)
Fixed “Missing bounds check for len(toOpenArray..)”
(#20954)
Fixed “Add warning for bare except: clause”
(#19580)
Fixed “Little Copyright notice inconsistency”
(#20906)
Fixed “std/deques: wrong result after calling shrink”
(#21278)
Fixed “io.readLine adds ‘\00’ char to the end”
(#21273)
Fixed “New JS mdoe issue: return + ref ints.”
(#21317)
Fixed “Bad codegen for passed var seq to proc returning array[] converted to seq with @”
(#21333)
Fixed “Templates allowed to use ambiguous identifier”
(#1027)
Fixed “Mutating a var parameter through a mutable view triggers SIGSEGV”
(#20422)
Fixed “macOS use SecRandomCopyBytes instead of getentropy”
(#20466)
Fixed “gcc error when constructing an object that has the same name in the same file name in 2 different directories”
(#20139)
The complete list of changes is available
here.</span>
                                &ensp;<span class="article-reading-time">(&hairsp;1
                                  min&hairsp;)</span>
                            </span>
                          </div>
                        </a>
                      </details>
                    </article>
                </section>
              </section>
            </li>
        </ul>
      </section>
      <section class="daily-content js-toggle-accordions-scope">
        <h2 class="daily-heading">
          <button
            class="daily-heading-toggle"
            data-action="toggle-accordions"
            title="Click to toggle the day, Ctrl + click to toggle all."
          >
            <time class="daily-heading-toggle__weekday js-offset-weekday" data-offset-date=2023-03-08 datetime="2023-03-08T00:00:00.000Z">2023-03-08</time>
            <time class="daily-heading-toggle__date js-offset-date" data-offset-date=2023-03-08 datetime="2023-03-08T00:00:00.000Z"></time>
          </button>
        </h2>
        <ul class="sources card">
            <li class="card__section">
              <section class="js-toggle-accordions-scope">
                <h3 class="source-heading">
                  <button
                    class="source-heading__name"
                    data-action="toggle-accordions"
                    title="Click to toggle the source, Ctrl + click to toggle all."
                  >matklad</button>
                  <a class="source-heading__link" href="https://matklad.github.io">Open</a>
                </h3>
                <section class="articles-per-source">
                    <article>
                      <details
                        class="article-expander"
                        data-action="toggle-native-accordion"
                        data-accordion-key="https://matklad.github.io/2023/03/08/an-engine-for-an-editor.html"
                        open
                      >
                        <summary class="article-expander__title">An Engine For An Editor</summary>
                        <a class="article-summary-link article-summary-box-outer" href="https://matklad.github.io/2023/03/08/an-engine-for-an-editor.html">
                          <div class="article-summary-box-inner media-object">
                            <span class="media-object__text">
                              <span>A common trope is how, if one wants to build a game, one should build a game, rather than a game engine, because it is all too easy to fall into a trap of building a generic solution, without getting to the game proper.
It seems to me that the situation with code editors is the opposite --- many people build editors, but few are building editor engines.
What&#x27;s an editor engine? A made up term I use to denote a thin waist the editor is build upon, the set of core concepts, entities and APIs which power the variety of editor&#x27;s components.
In this post, I will highlight Emacs&#x27; thin waist, which I think is worthy of imitation!</span>
                                &ensp;<span class="article-reading-time">(&hairsp;4
                                  min&hairsp;)</span>
                            </span>
                          </div>
                        </a>
                      </details>
                    </article>
                </section>
              </section>
            </li>
        </ul>
      </section>
      <section class="daily-content js-toggle-accordions-scope">
        <h2 class="daily-heading">
          <button
            class="daily-heading-toggle"
            data-action="toggle-accordions"
            title="Click to toggle the day, Ctrl + click to toggle all."
          >
            <time class="daily-heading-toggle__weekday js-offset-weekday" data-offset-date=2023-03-07 datetime="2023-03-07T20:39:08.000Z">2023-03-07</time>
            <time class="daily-heading-toggle__date js-offset-date" data-offset-date=2023-03-07 datetime="2023-03-07T20:39:08.000Z"></time>
          </button>
        </h2>
        <ul class="sources card">
            <li class="card__section">
              <section class="js-toggle-accordions-scope">
                <h3 class="source-heading">
                  <button
                    class="source-heading__name"
                    data-action="toggle-accordions"
                    title="Click to toggle the source, Ctrl + click to toggle all."
                  >Netflix TechBlog - Medium</button>
                  <a class="source-heading__link" href="https://netflixtechblog.com?source&#x3D;rss----2615bd06b42e---4">Open</a>
                </h3>
                <section class="articles-per-source">
                    <article>
                      <details
                        class="article-expander"
                        data-action="toggle-native-accordion"
                        data-accordion-key="https://medium.com/p/3c5c638740a8"
                        open
                      >
                        <summary class="article-expander__title">Data ingestion pipeline with Operation Management</summary>
                        <a class="article-summary-link article-summary-box-outer" href="https://netflixtechblog.com/data-ingestion-pipeline-with-operation-management-3c5c638740a8?source&#x3D;rss----2615bd06b42e---4">
                          <div class="article-summary-box-inner media-object">
                              <img src="https://miro.medium.com/v2/resize:fit:1200/0*c7aT8gHQfxOWmcdt" loading="lazy" class="media-object__media article-image" />
                            <span class="media-object__text">
                              <span>by Varun Sekhri, Meenakshi Jindal, Burak Bacioglu</span>
                                &ensp;<span class="article-reading-time">(&hairsp;12
                                  min&hairsp;)</span>
                            </span>
                          </div>
                        </a>
                      </details>
                    </article>
                </section>
              </section>
            </li>
        </ul>
      </section>
      <section class="daily-content js-toggle-accordions-scope">
        <h2 class="daily-heading">
          <button
            class="daily-heading-toggle"
            data-action="toggle-accordions"
            title="Click to toggle the day, Ctrl + click to toggle all."
          >
            <time class="daily-heading-toggle__weekday js-offset-weekday" data-offset-date=2023-03-05 datetime="2023-03-05T00:00:00.000Z">2023-03-05</time>
            <time class="daily-heading-toggle__date js-offset-date" data-offset-date=2023-03-05 datetime="2023-03-05T00:00:00.000Z"></time>
          </button>
        </h2>
        <ul class="sources card">
            <li class="card__section">
              <section class="js-toggle-accordions-scope">
                <h3 class="source-heading">
                  <button
                    class="source-heading__name"
                    data-action="toggle-accordions"
                    title="Click to toggle the source, Ctrl + click to toggle all."
                  >Posts on abhinavsarkar.net</button>
                  <a class="source-heading__link" href="https://abhinavsarkar.net/feed.atom">Open</a>
                </h3>
                <section class="articles-per-source">
                    <article>
                      <details
                        class="article-expander"
                        data-action="toggle-native-accordion"
                        data-accordion-key="https://notes.abhinavsarkar.net/2023/mastodon-context"
                        open
                      >
                        <summary class="article-expander__title">Pulling missing context of replied toots in Mastodon</summary>
                        <a class="article-summary-link article-summary-box-outer" href="https://notes.abhinavsarkar.net/2023/mastodon-context">
                          <div class="article-summary-box-inner media-object">
                            <span class="media-object__text">
                              <span>Mastodon is a decentralized social media platform that’s become increasingly popular in recent years. Unlike centralized social media platforms like Twitter and Facebook, Mastodon is run on separate individual servers, each with its own rules and community.
Mastodon servers are connected to each other via the ActivityPub protocol, which allows users to interact across servers. This means that users on one server can follow users on other servers and reply to their toots (Mastodon posts).
However, Mastodon does not automatically pull the context toots of the toots that users replied to from their original server. This means if you reply to a toot from another server, the all other replies to that toot may not be shown to you. See the discussion about this issue here.
This is a problem because it means that you may not be able to see the full context of a conversation, which can make it difficult to understand what’s going on.
The following script is a Python program that extracts the context toots of the toots that users replied to from their original server, and adds them to the local server. The script accomplishes this by first getting all local users that have posted a toot in the last x hours, where x is a configurable parameter. It then retrieves all replies to other users by those users in the last x hours. From there, it gets the context toots of those replies that are already present on the local server.
Then, for each reply toot, it finds the server and ID of the toot that it replied to. It then gets the context toots of those toots from their original server. Finally, it adds those context toots to the local server, which are not already present on the local server by removing the already present context toots from the list of toots to add.
It also caches the URLs of all toots that it has already added to the local server, so that it doesn’t add the same toot multiple times on subsequent runs.
You can run it like this:

ACCESS_TOKEN&#x3D;XXXX python3 pull_context.py fantastic.earth 24


The access token needs to have the following scopes enabled:
admin:read:accounts for getting the user IDs of the users.
read:statuses for getting the replies to other users.
read:search for adding the context toots to the local server.
The script should be run periodically, for example, every 15 minutes. Its only dependency is the requests library.
Click to see the script

  

#!/usr/bin/env python3

from datetime import datetime, timedelta
import itertools
import json
import os
import re
import sys
import requests


def pull_context(
    server,
    access_token,
    seen_urls,
    replied_toot_server_ids,
    reply_interval_hours,
):
    &quot;&quot;&quot;pull the context toots of toots user replied to, from their
    original server, and add them to the local server.&quot;&quot;&quot;
    user_ids &#x3D; get_active_user_ids(server, access_token, reply_interval_hours)
    reply_toots &#x3D; get_all_reply_toots(
        server, user_ids, access_token, seen_urls, reply_interval_hours
    )
    known_context_urls &#x3D; get_all_known_context_urls(server, reply_toots)
    seen_urls.update(known_context_urls)
    replied_toot_ids &#x3D; get_all_replied_toot_server_ids(
        server, reply_toots, replied_toot_server_ids
    )
    context_urls &#x3D; get_all_context_urls(server, replied_toot_ids)
    add_context_urls(server, access_token, context_urls, seen_urls)


def get_active_user_ids(server, access_token, reply_interval_hours):
    &quot;&quot;&quot;get all user IDs on the server that have posted a toot in the given
       time interval&quot;&quot;&quot;
    since &#x3D; datetime.now() - timedelta(days&#x3D;reply_interval_hours / 24 + 1)
    url &#x3D; f&quot;https://{server}/api/v1/admin/accounts&quot;
    resp &#x3D; requests.get(
        url, headers&#x3D;{&quot;Authorization&quot;: f&quot;Bearer {access_token}&quot;}, timeout&#x3D;5
    )
    if resp.status_code &#x3D;&#x3D; 200:
        for user in resp.json():
            last_status_at &#x3D; user[&quot;account&quot;][&quot;last_status_at&quot;]
            if last_status_at is not None:
                last_active &#x3D; datetime.strptime(last_status_at, &quot;%Y-%m-%d&quot;)
                if last_active &gt; since:
                    print(f&quot;Found active user: {user[&#x27;username&#x27;]}&quot;)
                    yield user[&quot;id&quot;]
    elif resp.status_code &#x3D;&#x3D; 403:
        raise Exception(
            f&quot;Error getting user IDs on server {server}. Status code: {resp.status_code}. &quot;
            &quot;Make sure you have the admin:read:accounts scope enabled for your access token.&quot;
        )
    else:
        raise Exception(
            f&quot;Error getting user IDs on server {server}. Status code: {resp.status_code}&quot;
        )


def get_all_reply_toots(
    server, user_ids, access_token, seen_urls, reply_interval_hours
):
    &quot;&quot;&quot;get all replies to other users by the given users in the last day&quot;&quot;&quot;
    replies_since &#x3D; datetime.now() - timedelta(hours&#x3D;reply_interval_hours)
    reply_toots &#x3D; list(
        itertools.chain.from_iterable(
            get_reply_toots(
                user_id, server, access_token, seen_urls, replies_since
            )
            for user_id in user_ids
        )
    )
    print(f&quot;Found {len(reply_toots)} reply toots&quot;)
    return reply_toots


def get_reply_toots(user_id, server, access_token, seen_urls, reply_since):
    &quot;&quot;&quot;get replies by the user to other users since the given date&quot;&quot;&quot;
    url &#x3D; f&quot;https://{server}/api/v1/accounts/{user_id}/statuses?exclude_replies&#x3D;false&amp;limit&#x3D;40&quot;

    try:
        resp &#x3D; requests.get(
            url, headers&#x3D;{&quot;Authorization&quot;: f&quot;Bearer {access_token}&quot;}, timeout&#x3D;5
        )
    except Exception as ex:
        print(
            f&quot;Error getting replies for user {user_id} on server {server}: {ex}&quot;
        )
        return []

    if resp.status_code &#x3D;&#x3D; 200:
        toots &#x3D; [
            toot
            for toot in resp.json()
            if toot[&quot;in_reply_to_id&quot;] is not None
            and toot[&quot;url&quot;] not in seen_urls
            and datetime.strptime(toot[&quot;created_at&quot;], &quot;%Y-%m-%dT%H:%M:%S.%fZ&quot;)
            &gt; reply_since
        ]
        for toot in toots:
            print(f&quot;Found reply toot: {toot[&#x27;url&#x27;]}&quot;)
        return toots
    elif resp.status_code &#x3D;&#x3D; 403:
        raise Exception(
            f&quot;Error getting replies for user {user_id} on server {server}. Status code: {resp.status_code}. &quot;
            &quot;Make sure you have the read:statuses scope enabled for your access token.&quot;
        )

    raise Exception(
        f&quot;Error getting replies for user {user_id} on server {server}. Status code: {resp.status_code}&quot;
    )


def get_all_known_context_urls(server, reply_toots):
    &quot;&quot;&quot;get the context toots of the given toots from their original server&quot;&quot;&quot;
    known_context_urls &#x3D; set(
        filter(
            lambda url: not url.startswith(f&quot;https://{server}/&quot;),
            itertools.chain.from_iterable(
                get_toot_context(*parse_mastodon_url(toot[&quot;url&quot;]), toot[&quot;url&quot;])
                for toot in reply_toots
            ),
        )
    )
    print(f&quot;Found {len(known_context_urls)} known context toots&quot;)
    return known_context_urls


def get_all_replied_toot_server_ids(
    server, reply_toots, replied_toot_server_ids
):
    &quot;&quot;&quot;get the server and ID of the toots the given toots replied to&quot;&quot;&quot;
    return filter(
        lambda x: x is not None,
        (
            get_replied_toot_server_id(server, toot, replied_toot_server_ids)
            for toot in reply_toots
        ),
    )


def get_replied_toot_server_id(server, toot, replied_toot_server_ids):
    &quot;&quot;&quot;get the server and ID of the toot the given toot replied to&quot;&quot;&quot;
    in_reply_to_id &#x3D; toot[&quot;in_reply_to_id&quot;]
    in_reply_to_account_id &#x3D; toot[&quot;in_reply_to_account_id&quot;]
    mentions &#x3D; [
        mention
        for mention in toot[&quot;mentions&quot;]
        if mention[&quot;id&quot;] &#x3D;&#x3D; in_reply_to_account_id
    ]
    if len(mentions) &#x3D;&#x3D; 0:
        return None

    mention &#x3D; mentions[0]

    o_url &#x3D; f&quot;https://{server}/@{mention[&#x27;acct&#x27;]}/{in_reply_to_id}&quot;
    if o_url in replied_toot_server_ids:
        return replied_toot_server_ids[o_url]

    url &#x3D; get_redirect_url(o_url)

    if url is None:
        return None

    match &#x3D; parse_mastodon_url(url)
    if match is not None:
        replied_toot_server_ids[o_url] &#x3D; (url, match)
        return (url, match)

    match &#x3D; parse_pleroma_url(url)
    if match is not None:
        replied_toot_server_ids[o_url] &#x3D; (url, match)
        return (url, match)

    print(f&quot;Error parsing toot URL {url}&quot;)
    replied_toot_server_ids[o_url] &#x3D; None
    return None


def parse_mastodon_url(url):
    &quot;&quot;&quot;parse a Mastodon URL and return the server and ID&quot;&quot;&quot;
    match &#x3D; re.match(
        r&quot;https://(?P&lt;server&gt;.*)/@(?P&lt;username&gt;.*)/(?P&lt;toot_id&gt;.*)&quot;, url
    )
    if match is not None:
        return (match.group(&quot;server&quot;), match.group(&quot;toot_id&quot;))
    return None


def parse_pleroma_url(url):
    &quot;&quot;&quot;parse a Pleroma URL and return the server and ID&quot;&quot;&quot;
    match &#x3D; re.match(r&quot;https://(?P&lt;server&gt;.*)/objects/(?P&lt;toot_id&gt;.*)&quot;, url)
    if match is not None:
        server &#x3D; match.group(&quot;server&quot;)
        url &#x3D; get_redirect_url(url)
        match &#x3D; re.match(r&quot;/notice/(?P&lt;toot_id&gt;.*)&quot;, url)
        if match is not None:
            return (server, match.group(&quot;toot_id&quot;))
        return None
    return None


def get_redirect_url(url):
    &quot;&quot;&quot;get the URL given URL redirects to&quot;&quot;&quot;
    try:
        resp &#x3D; requests.head(url, allow_redirects&#x3D;False, timeout&#x3D;5)
    except Exception as ex:
        print(f&quot;Error getting redirect URL for URL {url}. Exception: {ex}&quot;)
        return None

    if resp.status_code &#x3D;&#x3D; 200:
        return None
    elif resp.status_code &#x3D;&#x3D; 302:
        redirect_url &#x3D; resp.headers[&quot;Location&quot;]
        print(f&quot;Discovered redirect for URL {url}&quot;)
        return redirect_url
    else:
        print(
            f&quot;Error getting redirect URL for URL {url}. Status code: {resp.status_code}&quot;
        )
        return None


def get_all_context_urls(server, replied_toot_ids):
    &quot;&quot;&quot;get the URLs of the context toots of the given toots&quot;&quot;&quot;
    return filter(
        lambda url: not url.startswith(f&quot;https://{server}/&quot;),
        itertools.chain.from_iterable(
            get_toot_context(server, toot_id, url)
            for (url, (server, toot_id)) in replied_toot_ids
        ),
    )


def get_toot_context(server, toot_id, toot_url):
    &quot;&quot;&quot;get the URLs of the context toots of the given toot&quot;&quot;&quot;
    url &#x3D; f&quot;https://{server}/api/v1/statuses/{toot_id}/context&quot;
    try:
        resp &#x3D; requests.get(url, timeout&#x3D;5)
    except Exception as ex:
        print(f&quot;Error getting context for toot {toot_url}. Exception: {ex}&quot;)
        return []

    if resp.status_code &#x3D;&#x3D; 200:
        res &#x3D; resp.json()
        print(f&quot;Got context for toot {toot_url}&quot;)
        return (toot[&quot;url&quot;] for toot in (res[&quot;ancestors&quot;] + res[&quot;descendants&quot;]))

    print(
        f&quot;Error getting context for toot {toot_url}. Status code: {resp.status_code}&quot;
    )
    return []


def add_context_urls(server, access_token, context_urls, seen_urls):
    &quot;&quot;&quot;add the given toot URLs to the server&quot;&quot;&quot;
    count &#x3D; 0
    for url in context_urls:
        if url not in seen_urls:
            seen_urls.add(url)
            add_context_url(url, server, access_token)
            count +&#x3D; 1

    print(f&quot;Added {count} new context toots&quot;)


def add_context_url(url, server, access_token):
    &quot;&quot;&quot;add the given toot URL to the server&quot;&quot;&quot;
    search_url &#x3D; f&quot;https://{server}/api/v2/search?q&#x3D;{url}&amp;resolve&#x3D;true&amp;limit&#x3D;1&quot;

    try:
        resp &#x3D; requests.get(
            search_url,
            headers&#x3D;{&quot;Authorization&quot;: f&quot;Bearer {access_token}&quot;},
            timeout&#x3D;5,
        )
    except Exception as ex:
        print(
            f&quot;Error adding url {search_url} to server {server}. Exception: {ex}&quot;
        )
        return

    if resp.status_code &#x3D;&#x3D; 200:
        print(f&quot;Added context url {url}&quot;)
    elif resp.status_code &#x3D;&#x3D; 403:
        print(
            f&quot;Error adding url {search_url} to server {server}. Status code: {resp.status_code}. &quot;
            &quot;Make sure you have the read:search scope enabled for your access token.&quot;
        )
    else:
        print(
            f&quot;Error adding url {search_url} to server {server}. Status code: {resp.status_code}&quot;
        )


class OrderedSet:
    &quot;&quot;&quot;An ordered set implementation over a dict&quot;&quot;&quot;

    def __init__(self, iterable):
        self._dict &#x3D; {}
        for item in iterable:
            self.add(item)

    def add(self, item):
        if item not in self._dict:
            self._dict[item] &#x3D; None

    def update(self, iterable):
        for item in iterable:
            self.add(item)

    def __contains__(self, item):
        return item in self._dict

    def __iter__(self):
        return iter(self._dict)

    def __len__(self):
        return len(self._dict)


if __name__ &#x3D;&#x3D; &quot;__main__&quot;:
    HELP_MESSAGE &#x3D; &quot;&quot;&quot;
Usage: ACCESS_TOKEN&#x3D;XXXX python3 pull_context.py &lt;server&gt; &lt;reply_interval_in_hours&gt;

To run this script, set the ACCESS_TOKEN environment variable to your
Mastodon access token. The access token can be generated at
https://&lt;server&gt;/settings/applications, and must have read:search,
read:statuses and admin:read:accounts scopes.
&quot;&quot;&quot;

    try:
        ACCESS_TOKEN &#x3D; os.environ[&quot;ACCESS_TOKEN&quot;]
    except KeyError:
        print(&quot;ACCESS_TOKEN environment variable not set.&quot;)
        print(HELP_MESSAGE)
        sys.exit(1)

    if len(sys.argv) &lt; 3:
        print(HELP_MESSAGE)
        sys.exit(1)

    SERVER &#x3D; sys.argv[1]
    REPLY_INTERVAL_IN_HOURS &#x3D; int(sys.argv[2])
    SEEN_URLS_FILE &#x3D; &quot;seen_urls&quot;
    REPLIED_TOOT_SERVER_IDS_FILE &#x3D; &quot;replied_toot_server_ids&quot;

    SEEN_URLS &#x3D; OrderedSet([])
    if os.path.exists(SEEN_URLS_FILE):
        with open(SEEN_URLS_FILE, &quot;r&quot;, encoding&#x3D;&quot;utf-8&quot;) as f:
            SEEN_URLS &#x3D; OrderedSet(f.read().splitlines())

    REPLIED_TOOT_SERVER_IDS &#x3D; {}
    if os.path.exists(REPLIED_TOOT_SERVER_IDS_FILE):
        with open(REPLIED_TOOT_SERVER_IDS_FILE, &quot;r&quot;, encoding&#x3D;&quot;utf-8&quot;) as f:
            REPLIED_TOOT_SERVER_IDS &#x3D; json.load(f)

    pull_context(
        SERVER,
        ACCESS_TOKEN,
        SEEN_URLS,
        REPLIED_TOOT_SERVER_IDS,
        REPLY_INTERVAL_IN_HOURS,
    )

    with open(SEEN_URLS_FILE, &quot;w&quot;, encoding&#x3D;&quot;utf-8&quot;) as f:
        f.write(&quot;\n&quot;.join(list(SEEN_URLS)[:10000]))

    with open(REPLIED_TOOT_SERVER_IDS_FILE, &quot;w&quot;, encoding&#x3D;&quot;utf-8&quot;) as f:
        json.dump(dict(list(REPLIED_TOOT_SERVER_IDS.items())[:10000]), f)

  
I hope this is useful to someone else. I’m not a Python expert, so I’m sure there are some things that could be improved. I’m also not sure if this is the best way to do this, but it seems to work well enough for me.
You can like, repost, or comment on this post on Mastodon.</span>
                                &ensp;<span class="article-reading-time">(&hairsp;6
                                  min&hairsp;)</span>
                            </span>
                          </div>
                        </a>
                      </details>
                    </article>
                </section>
              </section>
            </li>
        </ul>
      </section>
      <section class="daily-content js-toggle-accordions-scope">
        <h2 class="daily-heading">
          <button
            class="daily-heading-toggle"
            data-action="toggle-accordions"
            title="Click to toggle the day, Ctrl + click to toggle all."
          >
            <time class="daily-heading-toggle__weekday js-offset-weekday" data-offset-date=2023-03-02 datetime="2023-03-02T00:00:00.000Z">2023-03-02</time>
            <time class="daily-heading-toggle__date js-offset-date" data-offset-date=2023-03-02 datetime="2023-03-02T00:00:00.000Z"></time>
          </button>
        </h2>
        <ul class="sources card">
            <li class="card__section">
              <section class="js-toggle-accordions-scope">
                <h3 class="source-heading">
                  <button
                    class="source-heading__name"
                    data-action="toggle-accordions"
                    title="Click to toggle the source, Ctrl + click to toggle all."
                  >Nim Programming Language</button>
                  <a class="source-heading__link" href="https://nim-lang.org//">Open</a>
                </h3>
                <section class="articles-per-source">
                    <article>
                      <details
                        class="article-expander"
                        data-action="toggle-native-accordion"
                        data-accordion-key="https://nim-lang.org//blog/2023/03/02/this-month-with-nim.html"
                        open
                      >
                        <summary class="article-expander__title">This Month with Nim: Feburary 2023</summary>
                        <a class="article-summary-link article-summary-box-outer" href="https://nim-lang.org//blog/2023/03/02/this-month-with-nim.html">
                          <div class="article-summary-box-inner media-object">
                              <img src="https://nim-lang.org/assets/img/twitter_banner.png" loading="lazy" class="media-object__media article-image" />
                            <span class="media-object__text">
                              <span>ttop
Author: Inv2004
System monitoring tool with TUI, historical data service and triggers
[x] Saving historical snapshots via systemd.timer or crontab
[x] Scroll via historical data
[x] TUI with critical values highlight
[x] External triggers (for notifications or other needs)
[x] Ascii graph of historical stats (via https://github.com/Yardanico/asciigraph)
[x] Temperature via sysfs
[x] User-space only, doesn’t require root permissions
[x] Static build
[x] Threads tree
ni18n
Author: Hein Thant
ni18n is a super simple and fast Nim macro for internationalization and localization.
No runtime lookup for translation since all translations are compiled down to Nim functions except we still have a runtime case statement for locale to call correct generated locale specific function.
import ni18n
…</span>
                                &ensp;<span class="article-reading-time">(&hairsp;3
                                  min&hairsp;)</span>
                            </span>
                          </div>
                        </a>
                      </details>
                    </article>
                </section>
              </section>
            </li>
        </ul>
      </section>

    <footer>
      <a class="footer-link" href="https://github.com/DhruvPatel01/feed-to-read/actions/runs/4498759817">
        <time id="build-timestamp" datetime="2023-03-23T08:59:31.283Z">2023-03-23T08:59:31.283Z</time>
      </a>
      <a class="footer-link" href="https://github.com/osmoscraft/osmosfeed">osmosfeed 1.15.1</a>
    </footer>
    <script src="index.js?v1.14.4"></script>
    <!-- %before-body-end.html% -->
  </body>

</html>