{
  "sources": [
    {
      "title": "null program",
      "feedUrl": "https://nullprogram.com/feed/",
      "siteUrl": "https://nullprogram.com",
      "articles": []
    },
    {
      "title": "Max Slater",
      "feedUrl": "https://thenumb.at/feed.xml",
      "siteUrl": "https://thenumbat.github.io/",
      "articles": [
        {
          "id": "https://thenumbat.github.io/Neural-Graphics/",
          "author": null,
          "description": "Neural fields have quickly become an interesting and useful application of machine learning to computer graphics. The fundamental idea is quite straightforward: we can use neural models to represent all sorts of digital signals, including images, light fields, signed distance functions, and volumes. Table of Contents Neural Networks Compression A Basic Network Activations Input Encodings Positional Encoding Instant Neural Graphics Primitives More Applications SDFs NeRFs References Neural Networks This post assumes a basic knowledge of neural networks, but let’s briefly recap how they work. A network with \\(n\\) inputs and \\(m\\) outputs approximates a function from \\(\\mathbb{R}^n \\mapsto \\mathbb{R}^m\\) by feeding its input through a sequence of layers. The simplest type of layer is a fully-connected layer, which encodes an affine transform: \\(x \\mapsto Ax + b\\), where \\(A\\) is the weight matrix and \\(b\\) is the bias vector. Layers other than the first (input) and last (output) are considered hidden layers, and can have arbitrary dimension. Each hidden layer is followed by applying a non-linear function \\(x \\mapsto f(x)\\), known as the activation. This additional function enables the network to encode non-linear behavior. For example, a network from \\(\\mathbb{R}^3 \\mapsto \\mathbb{R}^3\\), including two hidden layers of dimensions 5 and 4, can be visualized as follows. The connections between nodes represent weight matrix entries: Or equivalently in symbols (with dimensions): \\[\\begin{align*} h_{1(5)} &amp;= f(A_{1(5\\times3)}x_{(3)} + b_{1(5)})\\\\ h_{2(4)} &amp;= f(A_{2(4\\times5)}h_{1(5)} + b_{2(4)})\\\\ y_{(3)} &amp;= A_{3(3\\times4)}h_{2(4)} + b_{3(3)} \\end{align*}\\] This network has \\(5\\cdot3 + 5 + 4\\cdot5 + 4 + 3\\cdot4 + 3 = 59\\) parameters that define its weight matrices and bias vectors. The process of training finds values for these parameters such that the network approximates another function \\(g(x)\\). Suitable parameters are typically found via stochastic gradient descent. Training requires a loss function measuring how much \\(\\text{NN}(x)\\) differs from \\(g(x)\\) for all \\(x\\) in a training set. In practice, we might not know \\(g\\): given a large number of input-output pairs (e.g. images and their descriptions), we simply perform training and hope that the network will faithfully approximate \\(g\\) for inputs outside of the training set, too. We’ve only described the most basic kind of neural network: there’s a whole world of possibilities for new layers, activations, loss functions, optimization algorithms, and training paradigms. However, the fundamentals will suffice for this post. Compression When generalization is the goal, neural networks are typically under-constrained: they include far more parameters than strictly necessary to encode a function mapping the training set to its corresponding results. Through regularization, the training process hopes to use these ‘extra’ degrees of freedom to approximate the behavior of \\(g\\) outside the training set. However, insufficient regularization makes the network vulnerable to over-fitting—it may become extremely accurate on the training set, yet unable to handle new inputs. For this reason, researchers always evaluate neural networks on a test set of known data points that are excluded from the training process. However, in this post we’re actually going to optimize for over-fitting! Instead of approximating \\(g\\) on new inputs, we will only care about reproducing the right results on the training set. This approach allows us to use over-constrained networks for (lossy) data compression. Let’s say we have some data we want to compress into a neural network. If we can parameterize the input by assigning each value a unique identifier, that gives us a training set. For example, we could parameterize a list of numbers by their index: \\[[1, 1, 2, 5, 15, 52, 203] \\mapsto \\{ (0,1), (1,1), (2,2), (3,5), (4,15), (5,52), (6,203) \\}\\] …and train a network to associate the index \\(n\\) with the value \\(a[n]\\). To do so, we can simply define a loss function that measures the squared error of the result: \\((\\text{NN}(n) - a[n])^2\\). We only care that the network produces \\(a[n]\\) for \\(n\\) in 0 to 6, so we want to “over-fit” as much as possible. But, where’s the compression? If we make the network itself smaller than the data set—while being able to reproduce it—we can consider the network to be a compressed encoding of the data. For example, consider this photo of a numbat: Perth Zoo The image consists of 512x512 pixels with three channels (r,g,b) each. Hence, we could naively say it contains 786,432 parameters. If a network with fewer parameters can reproduce it, the network itself can be considered a compressed version of the image. More rigorously, each pixel can be encoded in 3 bytes of data, so we’d want to be able to store the network in fewer than 768 kilobytes—but reasoning about size on-disk would require getting into the weeds of mixed precision networks, so let’s only consider parameter count for now. A Basic Network Let’s train a network to encode the numbat. To create the data set, we will associate each pixel with its corresponding \\(x,y\\) coordinate. That means our training set will consist of 262,144 examples of the form \\((x,y) \\mapsto (r,g,b)\\). Before training, we’ll normalize the values such that \\(x,y \\in [-1,1]\\) and \\(r,g,b \\in [0,1]\\). \\[\\begin{align*} (-1.0000,-1.0000) &amp;\\mapsto (0.3098, 0.3686, 0.2471)\\\\ (-1.0000, -0.9961) &amp;\\mapsto (0.3059, 0.3686, 0.2471)\\\\ &amp;\\vdots\\\\ (0.9961, 0.9922) &amp;\\mapsto (0.3333, 0.4157, 0.3216)\\\\ (0.9961, 0.9961) &amp;\\mapsto (0.3412, 0.4039, 0.3216) \\end{align*}\\] Clearly, our network will need to be a function from \\(\\mathbb{R}^2 \\mapsto \\mathbb{R}^3\\), i.e. have two inputs and three outputs. Just going off intuition, we might include three hidden layers of dimension 128. This architecture will only have 33,795 parameters—far fewer than the image itself. \\[\\mathbb{R}^2 \\mapsto_{fc} \\mathbb{R}^{128} \\mapsto_{fc} \\mathbb{R}^{128} \\mapsto_{fc} \\mathbb{R}^{128} \\mapsto_{fc} \\mathbb{R}^3\\] Our activation function will be the standard non-linearity ReLU (rectified linear unit). Note that we only want to apply the activation after each hidden layer: we don’t want to clip our inputs or outputs by making them positive. \\[\\text{ReLU}(x) = \\max\\{x, 0\\}\\] Finally, our loss function will be the mean squared error between the network output and the expected color: \\[\\text{Loss}(x,y) = (\\text{NN}(x,y) - \\text{Image}_{xy})^2\\] Now, let’s train. We’ll do 100 passes over the full data set (100 epochs) with a batch size of 1024. After training, we’ll need some way to evaluate how well the network encoded our image. Hopefully, the network will now return the proper color given a pixel coordinate, so we can re-generate our image by simply evaluating the network at each pixel coordinate in the training set and normalizing the results. And, what do we get? After a bit of hyperparameter tweaking (learning rate, optimization schedule, epochs), we get… Epochs: 100 Well, that sort of worked—you can see the numbat taking shape. But unfortunately, by epoch 100 our loss isn’t consistently decreasing: more training won’t make the result significantly better. Activations So far, we’ve only used the ReLU activation function. Taking a look at the output image, we see a lot of lines: various activations jump from zero to positive across these boundaries. There’s nothing fundamentally wrong with that—given a suitably large network, we could still represent the exact image. However, it’s difficult to recover high-frequency detail by summing functions clipped at zero. Sigmoids Because we know the network should always return values in \\([0,1]\\), one easy improvement is adding a sigmoid activation to the output layer. Instead of teaching the network to directly output an intensity in \\([0,1]\\), this will allow the network to compute values in \\([-\\infty,\\infty]\\) that are deterministically mapped to a color in \\([0,1]\\). We’ll apply the logistic function: \\[\\text{Sigmoid}(x) = \\frac{1}{1 + e^{-x}}\\] For all future experiments, we’ll use this function as an output layer activation. Re-training the ReLU network: Epochs: 100 The result looks significantly better, but it’s still got a lot of line-like artifacts. What if we just use the sigmoid activation for the other hidden layers, too? Re-training again… Epochs: 100 That made it worse. The important observation here is that changing the activation function can have a significant effect on reproduction quality. Several papers have been published comparing different activation functions in this context, so let’s try some of those. Sinusoids Implicit Neural Representations with Periodic Activation Functions This paper explores the usage of periodic functions (i.e. \\(\\sin,\\cos\\)) as activations, finding them well suited for representing signals like images, audio, video, and distance fields. In particular, the authors note that the derivative of a sinusoidal network is also a sinusoidal network. This observation allows them to fit differential constraints in situations where ReLU and TanH-based models entirely fail to converge. The proposed form of periodic activations is simply \\(f(x) = \\sin(\\omega_0x)\\), where \\(\\omega_0\\) is a hyperparameter. Increasing \\(\\omega_0\\) should allow the network to encode higher frequency signals. Let’s try changing our activations to \\(f(x) = \\sin(2\\pi x)\\): Epochs: 100 Well, that’s better than the ReLU/Sigmoid networks, but still not very impressive—an unstable training process lost much of the low frequency data. It turns out that sinusoidal networks are quite sensitive to how we initialize the weight matrices. To account for the extra scaling by \\(\\omega_0\\), the paper proposes initializing weights beyond the first layer using the distribution \\(\\mathcal{U}(-\\frac{1}{\\omega_0}\\sqrt{\\frac{6}{\\text{fan_in}}}, \\frac{1}{\\omega_0}\\sqrt{\\frac{6}{\\text{fan_in}}})\\). Retraining with proper weight initialization: Epochs: 100 Now we’re getting somewhere—the output is quite recognizable. However, we’re still missing a lot of high frequency detail, and increasing \\(\\omega_0\\) much more starts to make training unstable. Gaussians Beyond Periodicity: Towards a Unifying Framework for Activations in Coordinate-MLPs This paper analyzes a variety of new activation functions, one of which is particularly suited for image reconstruction. It’s a gaussian of the form \\(f(x) = e^{\\frac{-x^2}{\\sigma^2}}\\), where \\(\\sigma\\) is a hyperparameter. Here, a smaller \\(\\sigma\\) corresponds to a higher bandwidth. The authors demonstrate good results using gaussian activations: reproduction as good as sinusoids, but without dependence on weight initialization or special input encodings (which we will discuss in the next section). So, let’s train our network using \\(f(x) = e^{-4x^2}\\): Epochs: 100 Well, that’s better than the initial sinusoidal network, but worse than the properly initialized one. However, something interesting happens if we scale up our input coordinates from \\([-1,1]\\) to \\([-16,16]\\): Epochs: 100 The center of the image is now reproduced almost perfectly, but the exterior still lacks detail. It seems that while gaussian activations are more robust to initialization, the distribution of inputs can still have a significant effect. Beyond the Training Set Although we’re only measuring how well each network reproduces the image, we might wonder what happens outside the training set. Luckily, our network is still just a function \\(\\mathbb{R}^2\\mapsto\\mathbb{R}^3\\), so we can evaluate it on a larger range of inputs to produce an “out of bounds” image. Another purported benefit of using gaussian activations is sensible behavior outside of the training set, so let’s compare. Evaluating each network on \\([-2,2]\\times[-2,2]\\): ReLU Sinusoid Gaussian That’s pretty cool—the gaussian network ends up representing a sort-of-edge-clamped extension of the image. The sinusoid turns into a low-frequency soup of common colors. The ReLU extension has little to do with the image content, and based on running training a few times, is very unstable. Input Encodings So far, our results are pretty cool, but not high fidelity enough to compete with the original image. Luckily, we can go further: recent research work on high-quality neural primitives has relied on not only better activation functions and training schemes, but new input encodings. When designing a network, an input encoding is essentially just a fixed-function initial layer that performs some interesting transformation before the fully-connected layers take over. It turns out that choosing a good initial transform can make a big difference. Positional Encoding The current go-to encoding is known as positional encoding, or otherwise positional embedding, or even sometimes fourier features. This encoding was first used in a graphics context by the original neural radiance fields paper. It takes the following form: \\[x \\mapsto \\left[x, \\sin(2^0\\pi x), \\cos(2^0\\pi x), \\dots, \\sin(2^L\\pi x), \\cos(2^L\\pi x) \\right]\\] Where \\(L\\) is a hyperparameter controlling bandwidth (higher \\(L\\), higher frequency signals). The unmodified input \\(x\\) may or may not be included in the output. When using this encoding, we transform our inputs by a hierarchy of sinusoids of increasing frequency. This brings to mind the fourier transform, hence the “fourier features” moniker, but note that we are not using the actual fourier transform of the training signal. So, let’s try adding a positional encoding to our network with \\(L=6\\). Our new network will have the following architecture: \\[\\mathbb{R}^2 \\mapsto_{enc6} \\mathbb{R}^{30} \\mapsto_{fc} \\mathbb{R}^{128} \\mapsto_{fc} \\mathbb{R}^{128} \\mapsto_{fc} \\mathbb{R}^{128} \\mapsto_{fc} \\mathbb{R}^3\\] Note that the sinusoids are applied element-wise to our two-dimensional \\(x\\), so we end up with \\(30\\) input dimensions. This means the first fully-connected layer will have \\(30\\cdot128\\) weights instead of \\(2\\cdot128\\), so we are adding some parameters. However, the additional weights don’t make a huge difference in of themselves. The ReLU network improves dramatically: Epochs: 100 The gaussian network also improves significantly, now only lacking some high frequency detail: Epochs: 100 Finally, the sinusoidal network… fails to converge! Reducing \\(\\omega_0\\) to \\(\\pi\\) lets training succeed, and produces the best result so far. However, the unstable training behavior is another indication that sinusoidal networks are particularly temperamental. Epochs: 100 As a point of reference, saving this model’s parameters in half precision (which does not degrade quality) requires only 76kb on disk. That’s smaller than a JPEG encoding of the image at ~111kb, though not quite as accurate. Unfortunately, it’s still missing some fine detail. Instant Neural Graphics Primitives Instant Neural Graphics Primitives with a Multiresolution Hash Encoding This paper made a splash when it was released early this year and eventually won a best paper award at SIGGRAPH 2022. It proposes a novel input encoding based on a learned multi-resolution hashing scheme. By combining their encoding with a fully-fused (i.e. single-shader) training and evaluation implementation, the authors are able to train networks representing gigapixel-scale images, SDF geometry, volumes, and radiance fields in near real-time. In this post, we’re only interested in the encoding, though I wouldn’t say no to instant training either. The Multiresolution Grid We begin by breaking up the domain into several square grids of increasing resolution. This hierarchy can be defined in any number of dimensions \\(d\\). Here, our domain will be two-dimensional: Note that the green and red grids also tile the full image; their full extent is omitted for clarity. The resolution of each grid is a constant multiple of the previous level, but note that the growth factor does not have to be two. In fact, the authors derive the scale after choosing the following three hyperparameters: \\[\\begin{align*} L &amp;:= \\text{Number of Levels} \\\\ N_{\\min} &amp;:= \\text{Coarsest Resolution} \\\\ N_{\\max} &amp;:= \\text{Finest Resolution} \\end{align*}\\] And compute the growth factor via: \\[b := \\exp\\left(\\frac{\\ln N_{\\max} - \\ln N_{\\min}}{L - 1}\\right)\\] Therefore, the resolution of grid level \\(\\ell\\) is \\(N_\\ell := \\lfloor N_\\min b^\\ell \\rfloor\\). Step 1 - Find Cells After choosing a hierarchy of grids, we can define the input transformation. We will assume the input \\(\\mathbf{x}\\) is given in \\([0,1]\\). For each level \\(\\ell\\), first scale \\(\\mathbf{x}\\) by \\(N_\\ell\\) and round up/down to find the cell containing \\(\\mathbf{x}\\). For example, when \\(N_\\ell = 2\\): Given the bounds of the cell containing \\(\\mathbf{x}\\), we can then compute the integer coordinates of each corner of the cell. In this example, we end up with four vertices: \\([0,0], [0,1], [1,0], [1,1]\\). Step 2 - Hash Coordinates We then hash each corner coordinate. The authors use the following function: \\[h(\\mathbf{x}) = \\bigoplus_{i=1}^d x_i\\pi_i\\] Where \\(\\oplus\\) denotes bit-wise exclusive-or and \\(\\pi_i\\) are large prime numbers. To improve cache coherence, the authors set \\(\\pi_1 = 1\\), as well as \\(\\pi_2 = 2654435761\\) and \\(\\pi_3 = 805459861\\). Because our domain is two-dimensional, we only need the first two coefficients: \\[h(x,y) = x \\oplus (2654435761 y)\\] The hash is then used as an index into a hash table. Each grid level has a corresponding hash table described by two more hyperparameters: \\[\\begin{align*} T &amp;:= \\text{Hash Table Slots} \\\\ F &amp;:= \\text{Features Per Slot} \\end{align*}\\] To map the hash to a slot in each level’s hash table, we simply modulo by \\(T\\). Each slot contains \\(F\\) learnable parameters. During training, we will backpropagate gradients all the way to the hash table entries, dynamically optimizing them to learn a good input encoding. What do we do about hash collisions? Nothing—the training process will automatically find an encoding that is robust to collisions. Unfortunately, this makes the encoding difficult to scale down to very small hash table sizes—eventually, simply too many grid points are assigned to each slot. Finally, the authors note that training is not particularly sensitive to how the hash table entries are initialized, but settle on an initial distribution \\(\\mathcal{U}(-0.0001,0.0001)\\). Step 3 - Interpolate Once we’ve retrieved the \\(2^d\\) hash table slots corresponding to the current grid cell, we linearly interpolate their values to define a result at \\(\\mathbf{x}\\) itself. In the two dimensional case, we use bi-linear interpolation: interpolate horizontally twice, then vertically once (or vice versa). The authors note that interpolating the discrete values is necessary for optimization with gradient descent: it makes the encoding function continuous. Step 4 - Concatenate At this point, we have mapped \\(\\mathbf{x}\\) to \\(L\\) different vectors of length \\(F\\)—one for each grid level \\(\\ell\\). To combine all of this information, we simply concatenate the vectors to form a single encoding of length \\(LF\\). The encoded result is then fed through a simple fully-connected neural network with ReLU activations (i.e. a multilayer perceptron). This network can be quite small: the authors use two hidden layers with dimension 64. Results Let’s implement the hash encoding and compare it with the earlier methods. The sinusoidal network with positional encoding might be hard to beat, given it uses only ~35k parameters. We will first choose \\(L = 16\\), \\(N_\\min = 16\\), \\(N_\\max = 256\\), \\(T = 1024\\), and \\(F = 2\\). Combined with a two hidden layer, dimension 64 MLP, these parameters define a model with 43,395 parameters. On disk, that’s about 90kb. Epochs: 30 That’s a good result, but not obviously better than we saw previously: when restricted to 1024-slot hash tables, the encoding can’t be perfect. However, the training process converges impressively quickly, producing a usable image after only three epochs and fully converging in 20-30. The sinusoidal network took over 80 epochs to converge, so fast convergence alone is a significant upside. The authors recommend a hash table size of \\(2^{14}-2^{24}\\): their use cases involve representing large volumetric data sets rather than 512x512 images. If we scale up our hash table to just 4096 entries (a parameter count of ~140k), the result at last becomes difficult to distinguish from the original image. The other techniques would have required even more parameters to achieve this level of quality. Epochs: 30 The larger hash maps result in a ~280kb model on disk, which, while much smaller than the uncompressed image, is over twice as large as an equivalent JPEG. The hash encoding really shines when representing much higher resolution images: at gigapixel scale it handily beats the other methods in reproduction quality, training time, and model size. (These results were compared against the reference implementation using the same parameters—the outputs were equivalent, except for my version being many times slower!) More Applications So far, we’ve only explored ways to represent images. But, as mentioned at the start, neural models can encode any dataset we can express as a function. In fact, current research work primarily focuses on representing geometry and light fields—images are the easy case. Neural SDFs One relevant way to represent surfaces is using signed distance functions, or SDFs. At every point \\(\\mathbf{x}\\), an SDF \\(f\\) computes the distance between \\(\\mathbf{x}\\) and the closest point on the surface. When \\(\\mathbf{x}\\) is inside the surface, \\(f\\) instead returns the negative distance. Hence, the surface is defined as the set of points such that \\(f(\\mathbf{x}) = 0\\), also known as the zero level set of \\(f\\). SDFs can be very simple to define and combine, and when combined with sphere tracing, can create remarkable results in a few lines of code. Hugh Kennedy Because SDFs are simply functions from position to distance, they’re easy to represent with neural models: all of the techniques we explored for images also apply when representing distance fields. Traditionally, graphics research and commercial tools have favored explicit geometric representations like triangle meshes—and broadly still do—but the advent of ML models is bringing implicit representations like SDFs back into the spotlight. Luckily, SDFs can be converted into meshes using methods like marching cubes and dual contouring, though this introduces discretization error. Implicit Neural Representations with Periodic Activation Functions When working with neural representations, one may not even require proper distances—it may be sufficient that \\(f(\\mathbf{x}) = 0\\) describes the surface. Techniques in this broader class are known as level set methods. Another SIGGRAPH 2022 best paper, Spelunking the Deep, defines relatively efficient closest point, collision, and ray intersection queries on arbitrary neural surfaces. Neural Radiance Fields In computer vision, another popular use case is modelling light fields. A light field can be encoded in many ways, but the model proposed in the original NeRF paper maps a 3D position and 2D angular direction to RGB radiance and volumetric density. This function provides enough information to synthesize images of the field using volumetric ray marching. (Basically, trace a ray in small steps, adding in radiance and attenuating based on density.) \\[x, y, z, \\theta, \\phi \\mapsto R, G, B, \\sigma\\] Because NeRFs are trained to match position and direction to radiance, one particularly successful use case has been using a set of 2D photographs to learn a 3D representation of a scene. Though NeRFs are not especially conducive to recovering geometry and materials, synthesizing images from entirely new angles is relatively easy—the model defines the full light field. NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis There has been an explosion of NeRF-related papers over the last two years, many of which choose different parameterizations of the light field. Some even ditch the neural encoding entirely. Using neural models to represent light fields also helped kickstart research on the more general topic of differentiable rendering, which seeks to separately recover scene parameters like geometry, materials, and lighting by reversing the rendering process via gradient descent. Neural Radiance Cache In real-time rendering, another exciting application is neural radiance caching. NRC brings NeRF concepts into the real-time ray-tracing world: a model is trained to encode a radiance field at runtime, dynamically learning from small batches of samples generated every frame. The resulting network is used as an intelligent cache to estimate incoming radiance for secondary rays—without recursive path tracing. Real-time Neural Radiance Caching for Path Tracing References This website attempts to collate all recent literature on neural fields: Neural Fields in Visual Computing and Beyond Papers referenced in this article: Implicit Neural Representations with Periodic Activation Functions Beyond Periodicity: Towards a Unifying Framework for Activations in Coordinate-MLPs NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis Instant Neural Graphics Primitives with a Multiresolution Hash Encoding Spelunking the Deep: Guaranteed Queries on General Neural Implicit Surfaces via Range Analysis PlenOctrees For Real-time Rendering of Neural Radiance Fields Real-time Neural Radiance Caching for Path Tracing More particularly cool papers: Neural Geometric Level of Detail: Real-time Rendering with Implicit 3D Shapes Learning Smooth Neural Functions via Lipschitz Regularization A Level Set Theory for Neural Implicit Evolution under Explicit Flows Implicit Neural Spatial Representations for Time-dependent PDEs Intrinsic Neural Fields: Learning Functions on Manifolds",
          "link": "https://thenumbat.github.io/Neural-Graphics/",
          "publishedOn": "2022-11-27T00:00:00.000Z",
          "wordCount": 3965,
          "title": "Exploring Neural Graphics Primitives",
          "imageUrl": null
        },
        {
          "id": "https://thenumbat.github.io/NYC-II/",
          "author": null,
          "description": "New York, NY, 2022",
          "link": "https://thenumbat.github.io/NYC-II/",
          "publishedOn": "2022-11-27T00:00:00.000Z",
          "wordCount": 82,
          "title": "NYC II",
          "imageUrl": null
        },
        {
          "id": "https://thenumbat.github.io/NYC-I/",
          "author": null,
          "description": "New York, NY, 2022",
          "link": "https://thenumbat.github.io/NYC-I/",
          "publishedOn": "2022-11-16T00:00:00.000Z",
          "wordCount": 82,
          "title": "NYC I",
          "imageUrl": null
        }
      ]
    },
    {
      "title": "Dennis Yurichev's blog",
      "feedUrl": "https://yurichev.com/blog/rss.xml",
      "siteUrl": "https://yurichev.org/",
      "articles": [
        {
          "id": "https://yurichev.org/tr/",
          "author": null,
          "description": "[Unix] Comparing CPU features with the help of cut/tr/diff/grep commands",
          "link": "https://yurichev.org/tr/",
          "publishedOn": "2022-12-11T00:00:00.000Z",
          "wordCount": 795,
          "title": "[Unix] Comparing CPU features with the help of cut/tr/diff/grep commands",
          "imageUrl": null
        },
        {
          "id": "https://yurichev.org/inline/",
          "author": null,
          "description": "[Pure C] Manual inlining",
          "link": "https://yurichev.org/inline/",
          "publishedOn": "2022-12-08T00:00:00.000Z",
          "wordCount": 337,
          "title": "[Pure C] Manual inlining",
          "imageUrl": null
        },
        {
          "id": "https://yurichev.org/power_wifi/",
          "author": null,
          "description": "[Украина] Лайфхак для работающих на ноутбуках в ТЦ и прочих кабаках",
          "link": "https://yurichev.org/power_wifi/",
          "publishedOn": "2022-12-06T00:00:00.000Z",
          "wordCount": 500,
          "title": "[Украина] Лайфхак для работающих на ноутбуках в ТЦ и прочих кабаках",
          "imageUrl": null
        },
        {
          "id": "https://yurichev.org/y2k/",
          "author": null,
          "description": "[Russian][Math] Проблема 2000 и модульная арифметика",
          "link": "https://yurichev.org/y2k/",
          "publishedOn": "2022-12-02T00:00:00.000Z",
          "wordCount": 216,
          "title": "[Russian][Math] Проблема 2000 и модульная арифметика",
          "imageUrl": null
        },
        {
          "id": "https://yurichev.org/eth1/",
          "author": null,
          "description": "[Crypto] Ethereum keystore - what is in it?",
          "link": "https://yurichev.org/eth1/",
          "publishedOn": "2022-12-01T00:00:00.000Z",
          "wordCount": 973,
          "title": "[Crypto] Ethereum keystore - what is in it?",
          "imageUrl": null
        },
        {
          "id": "https://yurichev.org/fuzzy_password/",
          "author": null,
          "description": "[Crypto] Fuzzy password",
          "link": "https://yurichev.org/fuzzy_password/",
          "publishedOn": "2022-11-30T00:00:00.000Z",
          "wordCount": 428,
          "title": "[Crypto] Fuzzy password",
          "imageUrl": null
        },
        {
          "id": "https://yurichev.org/clang/",
          "author": null,
          "description": "[Pure C] Clang is better than GCC",
          "link": "https://yurichev.org/clang/",
          "publishedOn": "2022-11-26T00:00:00.000Z",
          "wordCount": 452,
          "title": "[Pure C] Clang is better than GCC",
          "imageUrl": null
        },
        {
          "id": "https://yurichev.org/wordcloud/",
          "author": null,
          "description": "[Python] Wordclouds for your book library",
          "link": "https://yurichev.org/wordcloud/",
          "publishedOn": "2022-11-20T00:00:00.000Z",
          "wordCount": 258,
          "title": "[Python] Wordclouds for your book library",
          "imageUrl": null
        },
        {
          "id": "https://yurichev.org/dwm/",
          "author": null,
          "description": "[Unix] Logging your work using suckless dwm",
          "link": "https://yurichev.org/dwm/",
          "publishedOn": "2022-11-14T00:00:00.000Z",
          "wordCount": 993,
          "title": "[Unix] Logging your work using suckless dwm",
          "imageUrl": null
        },
        {
          "id": "https://yurichev.org/bool2/",
          "author": null,
          "description": "[Math][RevEng] Boolean algebra for noobs: stack alignment",
          "link": "https://yurichev.org/bool2/",
          "publishedOn": "2022-11-13T00:00:00.000Z",
          "wordCount": 426,
          "title": "[Math][RevEng] Boolean algebra for noobs: stack alignment",
          "imageUrl": null
        },
        {
          "id": "https://yurichev.org/selfref/",
          "author": null,
          "description": "Yet another self-referential joke about loser",
          "link": "https://yurichev.org/selfref/",
          "publishedOn": "2022-11-12T00:00:00.000Z",
          "wordCount": 231,
          "title": "Yet another self-referential joke about loser",
          "imageUrl": null
        }
      ]
    },
    {
      "title": "fasterthanli.me",
      "feedUrl": "https://fasterthanli.me/index.xml",
      "siteUrl": "https://fasterthanli.me",
      "articles": [
        {
          "id": "https://fasterthanli.me/series/advent-of-code-2022/part-9",
          "author": null,
          "description": "<p>The Advent of Code is not a sprint: it's a marathon: sometimes you've got to\nstop and smell the roses.</p>\n<div class=\"dialog bear\">\n<div class=\"dialog-head\" title=\"Cool bear says:\">\n<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 100 100\"><path d=\"M21.41 41.398c.148-1.787.337-3.572.574-5.351.369-2.772 4.393-11.308 5.624-12.882l.553-.706-1.344-.449a5.772 5.772 0 0 1-3.948-5.478 5.786 5.786 0 0 1 5.779-5.78 5.768 5.768 0 0 1 5.684 4.758l.249 1.396 1.231-.704a28.845 28.845 0 0 1 14.282-3.801c4.991 0 9.93 1.314 14.282 3.801l1.231.704.249-1.396a5.766 5.766 0 0 1 5.683-4.758 5.786 5.786 0 0 1 5.779 5.78 5.772 5.772 0 0 1-3.948 5.478l-1.344.449.552.705c1.911 2.44 2.55 2.541 3.563 5.599 1.422 4.293 2.923 7.964 2.923 12.61v49.875h2V41.373c0-6.519-2.008-12.717-5.819-18.005a7.768 7.768 0 0 0 4.072-6.836c0-4.29-3.49-7.78-7.779-7.78a7.766 7.766 0 0 0-7.3 5.1 30.834 30.834 0 0 0-14.146-3.45 30.834 30.834 0 0 0-14.146 3.45 7.766 7.766 0 0 0-7.301-5.1c-4.289 0-7.779 3.49-7.779 7.78a7.766 7.766 0 0 0 4.072 6.836c-6.681 9.271-5.976 21.037-5.992 32.034-.017 11.946.656 23.895.109 35.838l2.006-.001c.67 0-.17-33.831-.156-36.912.022-4.312.148-8.631.505-12.929z\"/><path d=\"M73.854 31.271v-4.494c0-.971-.79-1.76-1.761-1.76H55.235c-.604 0-1.3.869-1.778 1.905-.015 0-.027-.008-.042-.008H46.789c-.02 0-.037.011-.057.011-.478-1.037-1.175-1.908-1.779-1.908H28.094c-.971 0-1.761.79-1.761 1.76v4.494c0 4.477 3.643 8.119 8.119 8.119h5.12c4.265 0 7.734-3.47 7.734-7.734v-2.091H52.878v2.091c0 4.265 3.47 7.734 7.734 7.734h5.121c4.479-.001 8.121-3.643 8.121-8.119zm-41.713 3.192a1.387 1.387 0 0 1-1.917-.442 1.392 1.392 0 0 1 .442-1.917l8.403-5.255a1.392 1.392 0 0 1 1.475 2.359l-8.403 5.255zm11.163-2.18l-5.75 3.597a.95.95 0 0 1-1.313-.302.952.952 0 0 1 .303-1.312l5.75-3.597a.953.953 0 0 1 1.01 1.614zm15.168 1.935a1.387 1.387 0 0 1-1.917-.442 1.392 1.392 0 0 1 .442-1.917l8.403-5.255a1.392 1.392 0 0 1 1.475 2.359l-8.403 5.255zm4.909 1.561a.953.953 0 0 1-.506-1.759l5.75-3.597a.952.952 0 1 1 1.01 1.614l-5.75 3.597a.942.942 0 0 1-.504.145z\"/><path d=\"M59.705 45.232h-2c0 1.823-1.483 3.306-3.306 3.306s-3.306-1.483-3.306-3.306v-1.439h1.06a3.294 3.294 0 0 0 3.285-3.285v-.17c0-.259-.038-.508-.095-.749a3.291 3.291 0 0 0-2.665-2.482 3.243 3.243 0 0 0-.525-.053h-4.12c-.18 0-.353.025-.525.053a3.291 3.291 0 0 0-2.665 2.482 3.255 3.255 0 0 0-.095.749v.17a3.294 3.294 0 0 0 3.285 3.285h1.06v1.439a3.31 3.31 0 0 1-3.307 3.306 3.31 3.31 0 0 1-3.306-3.306h-2a5.312 5.312 0 0 0 5.306 5.306 5.293 5.293 0 0 0 4.306-2.229 5.294 5.294 0 0 0 4.306 2.229 5.313 5.313 0 0 0 5.307-5.306zM29.742 58.336a.703.703 0 0 0-1.406 0v5.368a.704.704 0 0 0 1.406 0v-5.368zM43.779 58.336a.703.703 0 0 0-1.406 0v5.368a.704.704 0 0 0 1.406 0v-5.368zM57.816 58.336a.703.703 0 0 0-1.407 0v5.368a.704.704 0 0 0 1.407 0v-5.368zM71.853 58.336a.704.704 0 0 0-1.407 0v5.368a.704.704 0 0 0 1.407 0v-5.368zM35.354 71.559v5.369a.703.703 0 0 0 1.406 0v-5.369a.703.703 0 0 0-1.406 0zM49.39 71.559v5.369a.704.704 0 0 0 1.407 0v-5.369a.703.703 0 0 0-1.407 0zM63.427 71.559v5.369a.704.704 0 0 0 1.407 0v-5.369a.703.703 0 0 0-1.407 0zM29.038 81.364a.703.703 0 0 0-.703.703v5.369a.703.703 0 0 0 1.406 0v-5.369a.703.703 0 0 0-.703-.703zM43.076 81.364a.703.703 0 0 0-.703.703v5.369a.703.703 0 0 0 1.406 0v-5.369a.705.705 0 0 0-.703-.703zM57.113 81.364a.704.704 0 0 0-.704.703v5.369a.704.704 0 0 0 1.407 0v-5.369a.705.705 0 0 0-.703-.703zM71.149 81.364a.703.703 0 0 0-.703.703v5.369a.703.703 0 0 0 1.407 0v-5.369a.705.705 0 0 0-.704-.703z\"/></svg>\n</div>\n<div class=\"dialog-text\">\n<p>I... what? That's not.. have you done a marathon before?</p>\n</div>\n</div>\n<p>No, and I haven't taken any creative writing classes either, I think you can\ntell. Anyway: Day 8 was a bit aggravating for me. In 2020 I gave up AoC after\nDay 14 I think, and then I skipped a year. It doesn't help that it overlaps some\nholidays and stuff, but!</p>",
          "link": "https://fasterthanli.me/series/advent-of-code-2022/part-9",
          "publishedOn": "2022-12-10T23:00:00.000Z",
          "wordCount": 4951,
          "title": "Day 9 (Advent of Code 2022)",
          "imageUrl": "https://fasterthanli.me/img/logo-square-2.png"
        },
        {
          "id": "https://fasterthanli.me/series/advent-of-code-2022/part-8",
          "author": null,
          "description": "<p>In the <a href=\"https://adventofcode.com/2022/day/8\">day 8 problem</a>, our input is a \nheight map:</p>\n<div class=\"code-block\"><pre class=\"code-block-inner\" data-lang=\"\">30373\n25512\n65332\n33549\n35390\n</pre></div>\n<p>This is a 5x5 grid, and every number denotes the height of a tree. For part 1,\nwe must find out how many trees are visible from the outside of the grid.</p>",
          "link": "https://fasterthanli.me/series/advent-of-code-2022/part-8",
          "publishedOn": "2022-12-09T14:00:00.000Z",
          "wordCount": 3358,
          "title": "Day 8 (Advent of Code 2022)",
          "imageUrl": "https://fasterthanli.me/img/logo-square-2.png"
        },
        {
          "id": "https://fasterthanli.me/series/advent-of-code-2022/part-7",
          "author": null,
          "description": "<p>The <a href=\"https://adventofcode.com/2022/day/7\">day 7 challenge</a> talks about\ntrees! File trees that is.</p>\n<p>The temptation to solve it before starting to write this article so I don't look\nsilly is high, but I'm explicitly not doing so, so that we can bang our\ncollective heads against any walls at the same time, and see how we can get\nout of it! Trees are serious business!</p>\n\n                        <h2>\n                            <a id=\"part-1\" class=\"anchor\" href=\"#part-1\">\n                                Part 1\n                            </a>\n                        </h2>",
          "link": "https://fasterthanli.me/series/advent-of-code-2022/part-7",
          "publishedOn": "2022-12-07T10:50:00.000Z",
          "wordCount": 6521,
          "title": "Day 7 (Advent of Code 2022)",
          "imageUrl": "https://fasterthanli.me/img/logo-square-2.png"
        },
        {
          "id": "https://fasterthanli.me/series/advent-of-code-2022/part-6",
          "author": null,
          "description": "<p>Today I am joining you from the relative discomfort of my living room (since my\nbetter half has commandeered the home office due to Way Too Many Calls) to\ntackle the <a href=\"https://adventofcode.com/2022/day/6\">day 6 challenge</a>, which I'm\nexcited about: maybe despite, maybe because of, the low-grade fever I'm under.</p>\n\n                        <h2>\n                            <a id=\"part-1\" class=\"anchor\" href=\"#part-1\">\n                                Part 1\n                            </a>\n                        </h2>",
          "link": "https://fasterthanli.me/series/advent-of-code-2022/part-6",
          "publishedOn": "2022-12-06T17:40:00.000Z",
          "wordCount": 3251,
          "title": "Day 6 (Advent of Code 2022)",
          "imageUrl": "https://fasterthanli.me/img/logo-square-2.png"
        },
        {
          "id": "https://fasterthanli.me/series/advent-of-code-2022/part-5",
          "author": null,
          "description": "<h2>\n                            <a id=\"part-1\" class=\"anchor\" href=\"#part-1\">\n                                Part 1\n                            </a>\n                        </h2>\n                        \n<p>The <a href=\"https://adventofcode.com/2022/day/5\">day 5 challenge</a> actually looks fun!</p>\n<p>Our input looks like this:</p>\n<div class=\"code-block\"><pre class=\"code-block-inner\" data-lang=\"\">    [D]    \n[N] [C]    \n[Z] [M] [P]\n 1   2   3 \n\nmove 1 from 2 to 1\nmove 3 from 1 to 3\nmove 2 from 2 to 1\nmove 1 from 1 to 2\n</pre></div>",
          "link": "https://fasterthanli.me/series/advent-of-code-2022/part-5",
          "publishedOn": "2022-12-05T21:29:00.000Z",
          "wordCount": 6535,
          "title": "Day 5 (Advent of Code 2022)",
          "imageUrl": "https://fasterthanli.me/img/logo-square-2.png"
        },
        {
          "id": "https://fasterthanli.me/series/advent-of-code-2022/part-4",
          "author": null,
          "description": "<h2>\n                            <a id=\"part-1\" class=\"anchor\" href=\"#part-1\">\n                                Part 1\n                            </a>\n                        </h2>\n                        \n<p>Let's tackle the <a href=\"https://adventofcode.com/2022/day/4\">day 4 challenge</a>!</p>\n<p>In this one, we get an input like this:</p>\n<div class=\"code-block\"><pre class=\"code-block-inner\" data-lang=\"\">2-4,6-8\n2-3,4-5\n5-7,7-9\n2-8,3-7\n6-6,4-6\n2-6,4-8\n</pre></div>",
          "link": "https://fasterthanli.me/series/advent-of-code-2022/part-4",
          "publishedOn": "2022-12-04T15:30:00.000Z",
          "wordCount": 2033,
          "title": "Day 4 (Advent of Code 2022)",
          "imageUrl": "https://fasterthanli.me/img/logo-square-2.png"
        },
        {
          "id": "https://fasterthanli.me/series/advent-of-code-2022/part-3",
          "author": null,
          "description": "<h2>\n                            <a id=\"part-1\" class=\"anchor\" href=\"#part-1\">\n                                Part 1\n                            </a>\n                        </h2>\n                        \n<p>I'm not sure where the <a href=\"https://adventofcode.com/2022/day/3\">day 3 challenge</a> is\ngoing, because the problem statement for the first part is kinda convoluted.</p>",
          "link": "https://fasterthanli.me/series/advent-of-code-2022/part-3",
          "publishedOn": "2022-12-03T09:50:00.000Z",
          "wordCount": 3842,
          "title": "Day 3 (Advent of Code 2022)",
          "imageUrl": "https://fasterthanli.me/img/logo-square-2.png"
        },
        {
          "id": "https://fasterthanli.me/series/advent-of-code-2022/part-2",
          "author": null,
          "description": "<h2>\n                            <a id=\"part-1\" class=\"anchor\" href=\"#part-1\">\n                                Part 1\n                            </a>\n                        </h2>\n                        \n<p>In the <a href=\"https://adventofcode.com/2022/day/2\">day 2 challenge</a>, we're playing\nRock Papers Scissors.</p>",
          "link": "https://fasterthanli.me/series/advent-of-code-2022/part-2",
          "publishedOn": "2022-12-02T16:10:00.000Z",
          "wordCount": 3045,
          "title": "Day 2 (Advent of Code 2022)",
          "imageUrl": "https://fasterthanli.me/img/logo-square-2.png"
        },
        {
          "id": "https://fasterthanli.me/series/advent-of-code-2022/part-1",
          "author": null,
          "description": "<p>Two years ago, I did part of <a href=\"/series/advent-of-code-2020\">Advent of Code 2020</a>\nusing the <a href=\"https://www.rust-lang.org/\">Rust</a> language. It was a lot of fun,\nso let's try it again!</p>\n\n                        <h2>\n                            <a id=\"the-problem-statement\" class=\"anchor\" href=\"#the-problem-statement\">\n                                The problem statement\n                            </a>\n                        </h2>",
          "link": "https://fasterthanli.me/series/advent-of-code-2022/part-1",
          "publishedOn": "2022-12-02T16:00:00.000Z",
          "wordCount": 8034,
          "title": "Day 1 (Advent of Code 2022)",
          "imageUrl": "https://fasterthanli.me/img/logo-square-2.png"
        },
        {
          "id": "https://fasterthanli.me/series/updating-fasterthanli-me-for-2022/part-8",
          "author": null,
          "description": "<p>This series has to end <em>somewhere</em>, so let's end it here!</p>\n<p>However, here is a list of some things I'd like to come back to:</p>\n\n                        <h2>\n                            <a id=\"bundling-typescript\" class=\"anchor\" href=\"#bundling-typescript\">\n                                Bundling & TypeScript\n                            </a>\n                        </h2>",
          "link": "https://fasterthanli.me/series/updating-fasterthanli-me-for-2022/part-8",
          "publishedOn": "2022-11-15T19:30:08.000Z",
          "wordCount": 1428,
          "title": "Cut for time",
          "imageUrl": "https://fasterthanli.me/img/logo-square-2.png"
        },
        {
          "id": "https://fasterthanli.me/series/updating-fasterthanli-me-for-2022/part-7",
          "author": null,
          "description": "<h2>\n                            <a id=\"async-trait-s-one-weird-type-ascription-trick\" class=\"anchor\" href=\"#async-trait-s-one-weird-type-ascription-trick\">\n                                <code>async_trait</code>'s one weird type ascription trick\n                            </a>\n                        </h2>\n                        \n<p>Now that I got <a href=\"part-6\">the Log in with GitHub feature</a> working, let's explore\nwhat this would've looked like with the <code>async_trait</code> crate.</p>",
          "link": "https://fasterthanli.me/series/updating-fasterthanli-me-for-2022/part-7",
          "publishedOn": "2022-11-15T19:30:07.000Z",
          "wordCount": 3331,
          "title": "Async fn in trait, for real this time",
          "imageUrl": "https://fasterthanli.me/img/logo-square-2.png"
        },
        {
          "id": "https://fasterthanli.me/series/updating-fasterthanli-me-for-2022/part-6",
          "author": null,
          "description": "<p>Because I started accepting donations via <a href=\"/donate\">GitHub Sponsors</a>, and\nbecause donating at the &quot;Silver&quot; tier or above gives you advance access to\narticles <em>and</em> your name in the credits, I need to interface with the GitHub\nAPI the same way I do the Patreon API.</p>",
          "link": "https://fasterthanli.me/series/updating-fasterthanli-me-for-2022/part-6",
          "publishedOn": "2022-11-15T19:30:06.000Z",
          "wordCount": 5199,
          "title": "Implementing \"Log in with GitHub\"",
          "imageUrl": "https://fasterthanli.me/img/logo-square-2.png"
        },
        {
          "id": "https://fasterthanli.me/series/updating-fasterthanli-me-for-2022/part-5",
          "author": null,
          "description": "<p>Now that my website is <a href=\"part-4\">deployed as a container image</a>, I wanted to give \n<a href=\"https://nixos.org/\">nix</a> a try. I'm still doing it the old-fashioned way right\nnow: with a <code>Dockerfile</code>, running <code>cargo</code> in a &quot;builder&quot; image, copying stuff\nout of there into a slimmer image (that still has an Ubuntu base, even though\n<a href=\"https://github.com/GoogleContainerTools/distroless\">distroless images</a> are a\nthing now).</p>\n\n                        <h2>\n                            <a id=\"but-why\" class=\"anchor\" href=\"#but-why\">\n                                But why?\n                            </a>\n                        </h2>",
          "link": "https://fasterthanli.me/series/updating-fasterthanli-me-for-2022/part-5",
          "publishedOn": "2022-11-15T19:30:05.000Z",
          "wordCount": 1554,
          "title": "Trying to use nix",
          "imageUrl": "https://fasterthanli.me/img/logo-square-2.png"
        },
        {
          "id": "https://fasterthanli.me/series/updating-fasterthanli-me-for-2022/part-4",
          "author": null,
          "description": "<div class=\"disclosure\">\n<p><strong>Disclosure</strong>: \nAlthough I no longer work for the company my website is hosted on, and this\narticle is written in way that mentions neither my previous or current hosting\nprovider: at the time of this writing, I don't pay for hosting.</p>\n</div>",
          "link": "https://fasterthanli.me/series/updating-fasterthanli-me-for-2022/part-4",
          "publishedOn": "2022-11-15T19:30:04.000Z",
          "wordCount": 2306,
          "title": "Deploying at the edge",
          "imageUrl": "https://fasterthanli.me/img/logo-square-2.png"
        },
        {
          "id": "https://fasterthanli.me/series/updating-fasterthanli-me-for-2022/part-3",
          "author": null,
          "description": "<h2>\n                            <a id=\"async-fn-in-trait-not\" class=\"anchor\" href=\"#async-fn-in-trait-not\">\n                                Async fn in trait... not\n                            </a>\n                        </h2>\n                        \n<p>I was planning on showing the in-progress <code>async_fn_in_trait</code> feature in the\ncontext of my website, but it turns out, I can't!</p>",
          "link": "https://fasterthanli.me/series/updating-fasterthanli-me-for-2022/part-3",
          "publishedOn": "2022-11-15T19:30:03.000Z",
          "wordCount": 1698,
          "title": "Async fn in trait... not",
          "imageUrl": "https://fasterthanli.me/img/logo-square-2.png"
        },
        {
          "id": "https://fasterthanli.me/series/updating-fasterthanli-me-for-2022/part-2",
          "author": null,
          "description": "<h2>\n                            <a id=\"falling-out-of-love-with-warp\" class=\"anchor\" href=\"#falling-out-of-love-with-warp\">\n                                Falling out of love with <code>warp</code>\n                            </a>\n                        </h2>\n                        \n<p>Back when I wrote this codebase, <a href=\"https://lib.rs/crates/warp\">warp</a> was the best\n/ only alternative for something relatively high-level on top of\n<a href=\"https://lib.rs/crates/hyper\">hyper</a>.</p>",
          "link": "https://fasterthanli.me/series/updating-fasterthanli-me-for-2022/part-2",
          "publishedOn": "2022-11-15T19:30:02.000Z",
          "wordCount": 4032,
          "title": "Migrating from warp to axum",
          "imageUrl": "https://fasterthanli.me/img/logo-square-2.png"
        },
        {
          "id": "https://fasterthanli.me/series/updating-fasterthanli-me-for-2022/part-1",
          "author": null,
          "description": "<h2>\n                            <a id=\"the-bleeding-edge-of-rustc-and-clippy\" class=\"anchor\" href=\"#the-bleeding-edge-of-rustc-and-clippy\">\n                                The bleeding edge of rustc and clippy\n                            </a>\n                        </h2>\n                        \n<p>Typically, you'd want a production application to use a stable version of Rust.\nAt the time of this writing, that's Rust 1.65.0, which stabilizes a bunch of\nlong-awaited features (GATs, let-else, MIR inlining, split debug info, etc.).</p>\n<div class=\"tip\">\n<div class=\"tip-header\">\n<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 100 100\"><path d=\"M21.41 41.398c.148-1.787.337-3.572.574-5.351.369-2.772 4.393-11.308 5.624-12.882l.553-.706-1.344-.449a5.772 5.772 0 0 1-3.948-5.478 5.786 5.786 0 0 1 5.779-5.78 5.768 5.768 0 0 1 5.684 4.758l.249 1.396 1.231-.704a28.845 28.845 0 0 1 14.282-3.801c4.991 0 9.93 1.314 14.282 3.801l1.231.704.249-1.396a5.766 5.766 0 0 1 5.683-4.758 5.786 5.786 0 0 1 5.779 5.78 5.772 5.772 0 0 1-3.948 5.478l-1.344.449.552.705c1.911 2.44 2.55 2.541 3.563 5.599 1.422 4.293 2.923 7.964 2.923 12.61v49.875h2V41.373c0-6.519-2.008-12.717-5.819-18.005a7.768 7.768 0 0 0 4.072-6.836c0-4.29-3.49-7.78-7.779-7.78a7.766 7.766 0 0 0-7.3 5.1 30.834 30.834 0 0 0-14.146-3.45 30.834 30.834 0 0 0-14.146 3.45 7.766 7.766 0 0 0-7.301-5.1c-4.289 0-7.779 3.49-7.779 7.78a7.766 7.766 0 0 0 4.072 6.836c-6.681 9.271-5.976 21.037-5.992 32.034-.017 11.946.656 23.895.109 35.838l2.006-.001c.67 0-.17-33.831-.156-36.912.022-4.312.148-8.631.505-12.929z\"/><path d=\"M73.854 31.271v-4.494c0-.971-.79-1.76-1.761-1.76H55.235c-.604 0-1.3.869-1.778 1.905-.015 0-.027-.008-.042-.008H46.789c-.02 0-.037.011-.057.011-.478-1.037-1.175-1.908-1.779-1.908H28.094c-.971 0-1.761.79-1.761 1.76v4.494c0 4.477 3.643 8.119 8.119 8.119h5.12c4.265 0 7.734-3.47 7.734-7.734v-2.091H52.878v2.091c0 4.265 3.47 7.734 7.734 7.734h5.121c4.479-.001 8.121-3.643 8.121-8.119zm-41.713 3.192a1.387 1.387 0 0 1-1.917-.442 1.392 1.392 0 0 1 .442-1.917l8.403-5.255a1.392 1.392 0 0 1 1.475 2.359l-8.403 5.255zm11.163-2.18l-5.75 3.597a.95.95 0 0 1-1.313-.302.952.952 0 0 1 .303-1.312l5.75-3.597a.953.953 0 0 1 1.01 1.614zm15.168 1.935a1.387 1.387 0 0 1-1.917-.442 1.392 1.392 0 0 1 .442-1.917l8.403-5.255a1.392 1.392 0 0 1 1.475 2.359l-8.403 5.255zm4.909 1.561a.953.953 0 0 1-.506-1.759l5.75-3.597a.952.952 0 1 1 1.01 1.614l-5.75 3.597a.942.942 0 0 1-.504.145z\"/><path d=\"M59.705 45.232h-2c0 1.823-1.483 3.306-3.306 3.306s-3.306-1.483-3.306-3.306v-1.439h1.06a3.294 3.294 0 0 0 3.285-3.285v-.17c0-.259-.038-.508-.095-.749a3.291 3.291 0 0 0-2.665-2.482 3.243 3.243 0 0 0-.525-.053h-4.12c-.18 0-.353.025-.525.053a3.291 3.291 0 0 0-2.665 2.482 3.255 3.255 0 0 0-.095.749v.17a3.294 3.294 0 0 0 3.285 3.285h1.06v1.439a3.31 3.31 0 0 1-3.307 3.306 3.31 3.31 0 0 1-3.306-3.306h-2a5.312 5.312 0 0 0 5.306 5.306 5.293 5.293 0 0 0 4.306-2.229 5.294 5.294 0 0 0 4.306 2.229 5.313 5.313 0 0 0 5.307-5.306zM29.742 58.336a.703.703 0 0 0-1.406 0v5.368a.704.704 0 0 0 1.406 0v-5.368zM43.779 58.336a.703.703 0 0 0-1.406 0v5.368a.704.704 0 0 0 1.406 0v-5.368zM57.816 58.336a.703.703 0 0 0-1.407 0v5.368a.704.704 0 0 0 1.407 0v-5.368zM71.853 58.336a.704.704 0 0 0-1.407 0v5.368a.704.704 0 0 0 1.407 0v-5.368zM35.354 71.559v5.369a.703.703 0 0 0 1.406 0v-5.369a.703.703 0 0 0-1.406 0zM49.39 71.559v5.369a.704.704 0 0 0 1.407 0v-5.369a.703.703 0 0 0-1.407 0zM63.427 71.559v5.369a.704.704 0 0 0 1.407 0v-5.369a.703.703 0 0 0-1.407 0zM29.038 81.364a.703.703 0 0 0-.703.703v5.369a.703.703 0 0 0 1.406 0v-5.369a.703.703 0 0 0-.703-.703zM43.076 81.364a.703.703 0 0 0-.703.703v5.369a.703.703 0 0 0 1.406 0v-5.369a.705.705 0 0 0-.703-.703zM57.113 81.364a.704.704 0 0 0-.704.703v5.369a.704.704 0 0 0 1.407 0v-5.369a.705.705 0 0 0-.703-.703zM71.149 81.364a.703.703 0 0 0-.703.703v5.369a.703.703 0 0 0 1.407 0v-5.369a.705.705 0 0 0-.704-.703z\"/></svg>\nCool bear's hot tip\n</div>\n\n</div>",
          "link": "https://fasterthanli.me/series/updating-fasterthanli-me-for-2022/part-1",
          "publishedOn": "2022-11-15T19:30:01.000Z",
          "wordCount": 3574,
          "title": "Cleaning up and upgrading third-party crates",
          "imageUrl": "https://fasterthanli.me/img/logo-square-2.png"
        }
      ]
    },
    {
      "title": "Netflix TechBlog - Medium",
      "feedUrl": "https://netflixtechblog.com/feed",
      "siteUrl": "https://netflixtechblog.com?source=rss----2615bd06b42e---4",
      "articles": [
        {
          "id": "https://medium.com/p/17440a9e141d",
          "author": "Netflix Technology Blog",
          "description": "by Jasmine Omeke, Obi-Ike Nwoke, Olek Gorajek",
          "link": "https://netflixtechblog.com/ready-to-go-sample-data-pipelines-with-dataflow-17440a9e141d?source=rss----2615bd06b42e---4",
          "publishedOn": "2022-12-04T00:10:21.000Z",
          "wordCount": 7457,
          "title": "Ready-to-go sample data pipelines with Dataflow",
          "imageUrl": "https://miro.medium.com/max/1200/1*4IalrwbpzJovyfmA8lMtyA.png"
        },
        {
          "id": "https://medium.com/p/5b8d032da09c",
          "author": "Netflix Technology Blog",
          "description": "by Christos G. Bampis, Li-Heng Chen and Zhi Li",
          "link": "https://netflixtechblog.com/for-your-eyes-only-improving-netflix-video-quality-with-neural-networks-5b8d032da09c?source=rss----2615bd06b42e---4",
          "publishedOn": "2022-11-17T21:08:00.000Z",
          "wordCount": 3958,
          "title": "For your eyes only: improving Netflix video quality with neural networks",
          "imageUrl": "https://miro.medium.com/max/1200/1*gn6_cn9dPKpu_YBykZvwDA.png"
        },
        {
          "id": "https://medium.com/p/31c3fc14ae59",
          "author": "Netflix Technology Blog",
          "description": "What’s needed in the art of match cutting is tools to help editors find shots that match well together, which is what we’ve started…",
          "link": "https://netflixtechblog.com/match-cutting-at-netflix-finding-cuts-with-smooth-visual-transitions-31c3fc14ae59?source=rss----2615bd06b42e---4",
          "publishedOn": "2022-11-17T16:37:01.000Z",
          "wordCount": 6579,
          "title": "Match Cutting at Netflix: Finding Cuts with Smooth Visual Transitions",
          "imageUrl": "https://miro.medium.com/max/854/0*vKMzII6YnjsnOda6"
        },
        {
          "id": "https://medium.com/p/d308ee43c079",
          "author": "Netflix Technology Blog",
          "description": "By: Peter Cioni (Netflix), Alex Schworer (Netflix), Mac Moore (Conductor Tech.), Rachel Kelley (AWS), Ranjit Raju (AWS)",
          "link": "https://netflixtechblog.com/helping-vfx-studios-pave-a-path-to-the-cloud-d308ee43c079?source=rss----2615bd06b42e---4",
          "publishedOn": "2022-11-15T17:03:55.000Z",
          "wordCount": 3235,
          "title": "Helping VFX studios pave a path to the cloud",
          "imageUrl": "https://miro.medium.com/max/1200/0*r72QIuSe00fdricw"
        },
        {
          "id": "https://medium.com/p/5067ac110bcd",
          "author": "Netflix Technology Blog",
          "description": "This blog series will be showing you how we use the power of machine learning to create stunning media at a global scale.",
          "link": "https://netflixtechblog.com/new-series-creating-media-with-machine-learning-5067ac110bcd?source=rss----2615bd06b42e---4",
          "publishedOn": "2022-11-11T17:26:39.000Z",
          "wordCount": 2842,
          "title": "New Series: Creating Media with Machine Learning",
          "imageUrl": "https://miro.medium.com/max/1200/1*T2e7vX6T2kyV7p9q8UD9GA.jpeg"
        },
        {
          "id": "https://medium.com/p/b0b4ef3be3f6",
          "author": "Netflix Technology Blog",
          "description": "By Soheil Esmaeilzadeh, Negin Salajegheh, Amir Ziai, Jeff Boote",
          "link": "https://netflixtechblog.com/machine-learning-for-fraud-detection-in-streaming-services-b0b4ef3be3f6?source=rss----2615bd06b42e---4",
          "publishedOn": "2022-11-11T17:26:15.000Z",
          "wordCount": 6293,
          "title": "Machine Learning for Fraud Detection in Streaming Services",
          "imageUrl": "https://miro.medium.com/max/938/0*Xz_msGGGvLslTL1H"
        }
      ]
    },
    {
      "title": "Nim Programming Language",
      "feedUrl": "https://nim-lang.org/feed.xml",
      "siteUrl": "/",
      "articles": [
        {
          "id": "/blog/2022/11/23/version-1610-released.html",
          "author": null,
          "description": "The Nim team is happy to announce version 1.6.10, our fifth patch release for\nNim 1.6.\nVersion 1.6.10 is a result of almost four months of hard work, and it contains\n29 commits,\nbringing some general improvements over 1.6.8.\nThis version brings OpenSSL 3 support\nto Nim 1.6, and large allocations and deallocations for ARC/ORC are now\nfaster.\nWe would recommend to all of our users to upgrade and use version 1.6.10.\nInstalling Nim 1.6\nNew users\nCheck out if the package manager of your OS already ships version 1.6.10 or\ninstall it as described here.\nExisting users\nIf you have installed a previous version of Nim using choosenim,\ngetting Nim 1.6.10 is as easy as:\n$ choosenim update stable\n\nAlternatively, you can download Nim 1.6.10 from\nour nightlies builds.\nDonating to Nim\nWe would like to encourage you to donate to Nim.\nThe donated money will be used to further improve Nim by creating bounties\nfor the most important bugfixes and features.\nYou can donate via:\nOpen Collective\nPatreon\nBountySource\nPayPal\nBitcoin: 1BXfuKM2uvoD6mbx4g5xM3eQhLzkCK77tJ\nIf you are a company, we also offer commercial support.\nBugfixes\nThese reported issues were fixed:\nFixed “–styleCheck:off does not work (and –styleCheck:hint is now the default?)”\n(#20397)\nFixed “dereferencing pointer to incomplete type error with gcc 9.4 with statics/cast”\n(#20141)\nFixed “strutils.find uses cstring optimization that stops after \\0”\n(#19500)\nFixed “Nimpretty mangles numeric literal procs”\n(#20553)\nFixed “Regression in proc symbol resolution; Error: attempting to call routine “\n(#18990)\nFixed “of operator doesn’t consider generics under orc/arc”\n(#20391)\nFixed ““incompatible type” when mixing float32 and cfloat in generics”\n(#19349)\nFixed “cannot generate code for: mSlice with toOpenArray”\n(#19969)\nFixed “-mm flag is ignored on latest Nim 1.7.1 be4bd8”\n(#20426)\nThe complete list of changes is available\nhere.",
          "link": "https://nim-lang.org/blog/2022/11/23/version-1610-released.html",
          "publishedOn": "2022-11-23T00:00:00.000Z",
          "wordCount": 436,
          "title": "Version 1.6.10 released",
          "imageUrl": "https://nim-lang.org/assets/img/twitter_banner.png"
        }
      ]
    }
  ],
  "cliVersion": "1.15.1"
}